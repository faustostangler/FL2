{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "import requests\n",
    "import yfinance as yf\n",
    "\n",
    "import string\n",
    "import random\n",
    "import unidecode\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DADOAS ABEERTOS DOWNLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL and report types\n",
    "base_url = \"https://dados.cvm.gov.br/dados/CIA_ABERTA/DOC/\"\n",
    "reports = [\"ITR\", \"DFP\"]\n",
    "\n",
    "data_folder = r\"D:\\Fausto Stangler\\Documentos\\Python\\FLY\\backend\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Container for the extracted data\n",
    "zip_files = []\n",
    "\n",
    "# Iterate over report types and collect .zip file URLs\n",
    "for report_type in reports:\n",
    "    url = f\"{base_url}{report_type}/DADOS/\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure the request is successful\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract all .zip links\n",
    "    zip_files.extend(\n",
    "        [\n",
    "            url + a[\"href\"]\n",
    "            for a in soup.find_all(\"a\", href=True)\n",
    "            if a[\"href\"].endswith(\".zip\")\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://dados.cvm.gov.br/dados/CIA_ABERTA/DOC/ITR/DADOS/itr_cia_aberta_2011.zip\n",
      "D:\\Fausto Stangler\\Documentos\\Python\\FLY\\backend\\data\\itr_cia_aberta_BPA_con_2011.csv\n",
      "D:\\Fausto Stangler\\Documentos\\Python\\FLY\\backend\\data\\itr_cia_aberta_BPA_ind_2011.csv\n",
      "D:\\Fausto Stangler\\Documentos\\Python\\FLY\\backend\\data\\itr_cia_aberta_BPP_con_2011.csv\n",
      "D:\\Fausto Stangler\\Documentos\\Python\\FLY\\backend\\data\\itr_cia_aberta_BPP_ind_2011.csv\n",
      "D:\\Fausto Stangler\\Documentos\\Python\\FLY\\backend\\data\\itr_cia_aberta_DFC_MD_con_2011.csv\n",
      "D:\\Fausto Stangler\\Documentos\\Python\\FLY\\backend\\data\\itr_cia_aberta_DFC_MD_ind_2011.csv\n",
      "D:\\Fausto Stangler\\Documentos\\Python\\FLY\\backend\\data\\itr_cia_aberta_DFC_MI_con_2011.csv\n",
      "D:\\Fausto Stangler\\Documentos\\Python\\FLY\\backend\\data\\itr_cia_aberta_DFC_MI_ind_2011.csv\n",
      "D:\\Fausto Stangler\\Documentos\\Python\\FLY\\backend\\data\\itr_cia_aberta_DMPL_con_2011.csv\n",
      "D:\\Fausto Stangler\\Documentos\\Python\\FLY\\backend\\data\\itr_cia_aberta_DMPL_ind_2011.csv\n",
      "D:\\Fausto Stangler\\Documentos\\Python\\FLY\\backend\\data\\itr_cia_aberta_DRA_con_2011.csv\n",
      "D:\\Fausto Stangler\\Documentos\\Python\\FLY\\backend\\data\\itr_cia_aberta_DRA_ind_2011.csv\n",
      "D:\\Fausto Stangler\\Documentos\\Python\\FLY\\backend\\data\\itr_cia_aberta_DRE_con_2011.csv\n",
      "D:\\Fausto Stangler\\Documentos\\Python\\FLY\\backend\\data\\itr_cia_aberta_DRE_ind_2011.csv\n",
      "D:\\Fausto Stangler\\Documentos\\Python\\FLY\\backend\\data\\itr_cia_aberta_DVA_con_2011.csv\n",
      "D:\\Fausto Stangler\\Documentos\\Python\\FLY\\backend\\data\\itr_cia_aberta_DVA_ind_2011.csv\n"
     ]
    }
   ],
   "source": [
    "level_skip = \"no_level\"\n",
    "keywords = [\"ind\", \"con\"]\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "for zip_url in zip_files:\n",
    "    print(f\"{zip_url}\")\n",
    "    try:\n",
    "        # Determine a cache file path based on the URL\n",
    "        cached_file_path = os.path.join(data_folder, os.path.basename(zip_url))\n",
    "\n",
    "        # Check if the file is already cached\n",
    "        if os.path.exists(cached_file_path):\n",
    "            with open(cached_file_path, \"rb\") as cached_file:\n",
    "                content = cached_file.read()\n",
    "        else:\n",
    "            print(f\"Downloading: {zip_url}\")\n",
    "            # Request the .zip file\n",
    "            response = requests.get(zip_url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Save to cache after validation\n",
    "            content = response.content\n",
    "            with open(cached_file_path, \"wb\") as cached_file:\n",
    "                cached_file.write(content)\n",
    "\n",
    "        # Open the .zip file\n",
    "        with zipfile.ZipFile(io.BytesIO(content)) as z:\n",
    "            # List all files in the ZIP archive\n",
    "            file_names = z.namelist()\n",
    "            for file_name in file_names:\n",
    "                try:\n",
    "                    # Check if the file is a CSV or another readable format\n",
    "                    if file_name.endswith(\".csv\") and any(\n",
    "                        keyword in file_name for keyword in keywords\n",
    "                    ):\n",
    "                        # Parse the filename for year, level, and type\n",
    "                        parts = file_name.split(\"_\")\n",
    "                        year = parts[-1].split(\".\")[0]  # Extract the year\n",
    "                        level = parts[-2] if parts[-2] in [\"con\", \"ind\"] else level_skip\n",
    "                        doc_type = parts[-3] if level in [\"con\", \"ind\"] else parts[-2]\n",
    "\n",
    "                        # Construct the path to the file\n",
    "                        file_path = os.path.join(data_folder, file_name)\n",
    "                        print(f\"{file_path}\")\n",
    "\n",
    "                        # Read the CSV into a DataFrame\n",
    "                        with z.open(file_name) as f:\n",
    "                            # df = pd.read_csv(f, encoding='latin1', sep=';', error_bad_lines=False)\n",
    "                            df = pd.read_csv(\n",
    "                                f, encoding=\"latin1\", sep=\";\"\n",
    "                            )  # Read CSV file\n",
    "\n",
    "                            # Initialize nested dictionaries as needed\n",
    "                            if year not in dataframes:\n",
    "                                dataframes[year] = {}\n",
    "                            if doc_type not in dataframes[year]:\n",
    "                                dataframes[year][doc_type] = {}\n",
    "\n",
    "                            # Store the DataFrame in the appropriate place\n",
    "                            if level != level_skip:\n",
    "                                dataframes[year][doc_type][level] = df\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to process {file_name}: {e}\")\n",
    "\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {zip_url}: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['BPA', 'BPP', 'MD', 'MI', 'DMPL', 'DRA', 'DRE', 'DVA'])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes[\"2011\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataframes[\"2011\"][\"BPA\"][\"con\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNPJ_CIA</th>\n",
       "      <th>DT_REFER</th>\n",
       "      <th>VERSAO</th>\n",
       "      <th>DENOM_CIA</th>\n",
       "      <th>CD_CVM</th>\n",
       "      <th>GRUPO_DFP</th>\n",
       "      <th>MOEDA</th>\n",
       "      <th>ESCALA_MOEDA</th>\n",
       "      <th>ORDEM_EXERC</th>\n",
       "      <th>DT_FIM_EXERC</th>\n",
       "      <th>CD_CONTA</th>\n",
       "      <th>DS_CONTA</th>\n",
       "      <th>VL_CONTA</th>\n",
       "      <th>ST_CONTA_FIXA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00.001.180/0001-26</td>\n",
       "      <td>2011-03-31</td>\n",
       "      <td>1</td>\n",
       "      <td>CENTRAIS ELET BRAS S.A. - ELETROBRAS</td>\n",
       "      <td>2437</td>\n",
       "      <td>DF Consolidado - Balan√ßo Patrimonial Ativo</td>\n",
       "      <td>REAL</td>\n",
       "      <td>MIL</td>\n",
       "      <td>√öLTIMO</td>\n",
       "      <td>2011-03-31</td>\n",
       "      <td>1</td>\n",
       "      <td>Ativo Total</td>\n",
       "      <td>149751700.0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00.001.180/0001-26</td>\n",
       "      <td>2011-03-31</td>\n",
       "      <td>1</td>\n",
       "      <td>CENTRAIS ELET BRAS S.A. - ELETROBRAS</td>\n",
       "      <td>2437</td>\n",
       "      <td>DF Consolidado - Balan√ßo Patrimonial Ativo</td>\n",
       "      <td>REAL</td>\n",
       "      <td>MIL</td>\n",
       "      <td>√öLTIMO</td>\n",
       "      <td>2011-03-31</td>\n",
       "      <td>1.01</td>\n",
       "      <td>Ativo Circulante</td>\n",
       "      <td>34736318.0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00.001.180/0001-26</td>\n",
       "      <td>2011-03-31</td>\n",
       "      <td>1</td>\n",
       "      <td>CENTRAIS ELET BRAS S.A. - ELETROBRAS</td>\n",
       "      <td>2437</td>\n",
       "      <td>DF Consolidado - Balan√ßo Patrimonial Ativo</td>\n",
       "      <td>REAL</td>\n",
       "      <td>MIL</td>\n",
       "      <td>√öLTIMO</td>\n",
       "      <td>2011-03-31</td>\n",
       "      <td>1.01.01</td>\n",
       "      <td>Caixa e Equivalentes de Caixa</td>\n",
       "      <td>12358365.0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00.001.180/0001-26</td>\n",
       "      <td>2011-03-31</td>\n",
       "      <td>1</td>\n",
       "      <td>CENTRAIS ELET BRAS S.A. - ELETROBRAS</td>\n",
       "      <td>2437</td>\n",
       "      <td>DF Consolidado - Balan√ßo Patrimonial Ativo</td>\n",
       "      <td>REAL</td>\n",
       "      <td>MIL</td>\n",
       "      <td>√öLTIMO</td>\n",
       "      <td>2011-03-31</td>\n",
       "      <td>1.01.01.01</td>\n",
       "      <td>Caixa e equivalente de caixa</td>\n",
       "      <td>9763580.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00.001.180/0001-26</td>\n",
       "      <td>2011-03-31</td>\n",
       "      <td>1</td>\n",
       "      <td>CENTRAIS ELET BRAS S.A. - ELETROBRAS</td>\n",
       "      <td>2437</td>\n",
       "      <td>DF Consolidado - Balan√ßo Patrimonial Ativo</td>\n",
       "      <td>REAL</td>\n",
       "      <td>MIL</td>\n",
       "      <td>√öLTIMO</td>\n",
       "      <td>2011-03-31</td>\n",
       "      <td>1.01.01.02</td>\n",
       "      <td>Caixa restrito</td>\n",
       "      <td>2594785.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135861</th>\n",
       "      <td>97.837.181/0001-47</td>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>1</td>\n",
       "      <td>DEXCO S.A.</td>\n",
       "      <td>21091</td>\n",
       "      <td>DF Consolidado - Balan√ßo Patrimonial Ativo</td>\n",
       "      <td>REAL</td>\n",
       "      <td>MIL</td>\n",
       "      <td>√öLTIMO</td>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>1.02.04.01.03</td>\n",
       "      <td>Softwares, marcas e patentes</td>\n",
       "      <td>28262.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135863</th>\n",
       "      <td>97.837.181/0001-47</td>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>1</td>\n",
       "      <td>DEXCO S.A.</td>\n",
       "      <td>21091</td>\n",
       "      <td>DF Consolidado - Balan√ßo Patrimonial Ativo</td>\n",
       "      <td>REAL</td>\n",
       "      <td>MIL</td>\n",
       "      <td>√öLTIMO</td>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>1.02.04.02</td>\n",
       "      <td>Goodwill</td>\n",
       "      <td>226819.0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135865</th>\n",
       "      <td>97.837.181/0001-47</td>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>1</td>\n",
       "      <td>DEXCO S.A.</td>\n",
       "      <td>21091</td>\n",
       "      <td>DF Consolidado - Balan√ßo Patrimonial Ativo</td>\n",
       "      <td>REAL</td>\n",
       "      <td>MIL</td>\n",
       "      <td>√öLTIMO</td>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>1.02.04.02.01</td>\n",
       "      <td>Goodwill na associa√ß√£o da Satipel em 2009</td>\n",
       "      <td>187573.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135867</th>\n",
       "      <td>97.837.181/0001-47</td>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>1</td>\n",
       "      <td>DEXCO S.A.</td>\n",
       "      <td>21091</td>\n",
       "      <td>DF Consolidado - Balan√ßo Patrimonial Ativo</td>\n",
       "      <td>REAL</td>\n",
       "      <td>MIL</td>\n",
       "      <td>√öLTIMO</td>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>1.02.04.02.02</td>\n",
       "      <td>Goodwill na aquisi√ß√£o Cer√¢mica Monte Carlo em ...</td>\n",
       "      <td>22154.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135869</th>\n",
       "      <td>97.837.181/0001-47</td>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>1</td>\n",
       "      <td>DEXCO S.A.</td>\n",
       "      <td>21091</td>\n",
       "      <td>DF Consolidado - Balan√ßo Patrimonial Ativo</td>\n",
       "      <td>REAL</td>\n",
       "      <td>MIL</td>\n",
       "      <td>√öLTIMO</td>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>1.02.04.02.03</td>\n",
       "      <td>Goodwill na aquisi√ß√£o da Deca Nordeste em 2011</td>\n",
       "      <td>17092.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68055 rows √ó 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  CNPJ_CIA    DT_REFER  VERSAO  \\\n",
       "1       00.001.180/0001-26  2011-03-31       1   \n",
       "3       00.001.180/0001-26  2011-03-31       1   \n",
       "5       00.001.180/0001-26  2011-03-31       1   \n",
       "7       00.001.180/0001-26  2011-03-31       1   \n",
       "9       00.001.180/0001-26  2011-03-31       1   \n",
       "...                    ...         ...     ...   \n",
       "135861  97.837.181/0001-47  2011-09-30       1   \n",
       "135863  97.837.181/0001-47  2011-09-30       1   \n",
       "135865  97.837.181/0001-47  2011-09-30       1   \n",
       "135867  97.837.181/0001-47  2011-09-30       1   \n",
       "135869  97.837.181/0001-47  2011-09-30       1   \n",
       "\n",
       "                                   DENOM_CIA  CD_CVM  \\\n",
       "1       CENTRAIS ELET BRAS S.A. - ELETROBRAS    2437   \n",
       "3       CENTRAIS ELET BRAS S.A. - ELETROBRAS    2437   \n",
       "5       CENTRAIS ELET BRAS S.A. - ELETROBRAS    2437   \n",
       "7       CENTRAIS ELET BRAS S.A. - ELETROBRAS    2437   \n",
       "9       CENTRAIS ELET BRAS S.A. - ELETROBRAS    2437   \n",
       "...                                      ...     ...   \n",
       "135861                            DEXCO S.A.   21091   \n",
       "135863                            DEXCO S.A.   21091   \n",
       "135865                            DEXCO S.A.   21091   \n",
       "135867                            DEXCO S.A.   21091   \n",
       "135869                            DEXCO S.A.   21091   \n",
       "\n",
       "                                         GRUPO_DFP MOEDA ESCALA_MOEDA  \\\n",
       "1       DF Consolidado - Balan√ßo Patrimonial Ativo  REAL          MIL   \n",
       "3       DF Consolidado - Balan√ßo Patrimonial Ativo  REAL          MIL   \n",
       "5       DF Consolidado - Balan√ßo Patrimonial Ativo  REAL          MIL   \n",
       "7       DF Consolidado - Balan√ßo Patrimonial Ativo  REAL          MIL   \n",
       "9       DF Consolidado - Balan√ßo Patrimonial Ativo  REAL          MIL   \n",
       "...                                            ...   ...          ...   \n",
       "135861  DF Consolidado - Balan√ßo Patrimonial Ativo  REAL          MIL   \n",
       "135863  DF Consolidado - Balan√ßo Patrimonial Ativo  REAL          MIL   \n",
       "135865  DF Consolidado - Balan√ßo Patrimonial Ativo  REAL          MIL   \n",
       "135867  DF Consolidado - Balan√ßo Patrimonial Ativo  REAL          MIL   \n",
       "135869  DF Consolidado - Balan√ßo Patrimonial Ativo  REAL          MIL   \n",
       "\n",
       "       ORDEM_EXERC DT_FIM_EXERC       CD_CONTA  \\\n",
       "1           √öLTIMO   2011-03-31              1   \n",
       "3           √öLTIMO   2011-03-31           1.01   \n",
       "5           √öLTIMO   2011-03-31        1.01.01   \n",
       "7           √öLTIMO   2011-03-31     1.01.01.01   \n",
       "9           √öLTIMO   2011-03-31     1.01.01.02   \n",
       "...            ...          ...            ...   \n",
       "135861      √öLTIMO   2011-09-30  1.02.04.01.03   \n",
       "135863      √öLTIMO   2011-09-30     1.02.04.02   \n",
       "135865      √öLTIMO   2011-09-30  1.02.04.02.01   \n",
       "135867      √öLTIMO   2011-09-30  1.02.04.02.02   \n",
       "135869      √öLTIMO   2011-09-30  1.02.04.02.03   \n",
       "\n",
       "                                                 DS_CONTA     VL_CONTA  \\\n",
       "1                                             Ativo Total  149751700.0   \n",
       "3                                        Ativo Circulante   34736318.0   \n",
       "5                           Caixa e Equivalentes de Caixa   12358365.0   \n",
       "7                            Caixa e equivalente de caixa    9763580.0   \n",
       "9                                          Caixa restrito    2594785.0   \n",
       "...                                                   ...          ...   \n",
       "135861                       Softwares, marcas e patentes      28262.0   \n",
       "135863                                           Goodwill     226819.0   \n",
       "135865          Goodwill na associa√ß√£o da Satipel em 2009     187573.0   \n",
       "135867  Goodwill na aquisi√ß√£o Cer√¢mica Monte Carlo em ...      22154.0   \n",
       "135869     Goodwill na aquisi√ß√£o da Deca Nordeste em 2011      17092.0   \n",
       "\n",
       "       ST_CONTA_FIXA  \n",
       "1                  S  \n",
       "3                  S  \n",
       "5                  S  \n",
       "7                  N  \n",
       "9                  N  \n",
       "...              ...  \n",
       "135861             N  \n",
       "135863             S  \n",
       "135865             N  \n",
       "135867             N  \n",
       "135869             N  \n",
       "\n",
       "[68055 rows x 14 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[\"ORDEM_EXERC\"] != \"PEN√öLTIMO\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import content as df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process df to match statements columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as some appropriate format, maybe check nsd numbers or something yet to be defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENET DOWNLOAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get new NSD ENET list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all nsd and filter only ITR and DFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all saved-reports-nsd and exclude saved-reports-nsd from all-nsd list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download enet nsd-new reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open nsd page, get info and generate enet download link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download enet zip, open and read xls and delete zip and other files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### process and clean xls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### incremental save nsd reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update saved-reports-nsd list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Financial Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_dfs_types(\n",
    "    df,\n",
    "    source_type=\"Dados da Empresa\",\n",
    "    target_types=[\"DFs Consolidadas\", \"DFs Individuais\"],\n",
    "):\n",
    "    \"\"\"\n",
    "    Duplica condicionalmente as linhas de um tipo espec√≠fico para outros tipos,\n",
    "    baseando-se na exist√™ncia pr√©via desses tipos para a mesma empresa e quarter.\n",
    "\n",
    "    Par√¢metros:\n",
    "    - df (pd.DataFrame): DataFrame original contendo os dados financeiros.\n",
    "    - source_type (str): O tipo de linha que ser√° duplicado. Padr√£o: 'Dados da Empresa'.\n",
    "    - target_types (list of str): Lista dos tipos para os quais as linhas ser√£o duplicadas.\n",
    "                                   Padr√£o: ['DFs Consolidadas', 'DFs Individuais'].\n",
    "\n",
    "    Retorna:\n",
    "    - pd.DataFrame: DataFrame atualizado com as duplica√ß√µes condicionais.\n",
    "    \"\"\"\n",
    "\n",
    "    # Passo 1: Identificar combina√ß√µes existentes de company_name e quarter para cada target_type\n",
    "    existing_combinations = {}\n",
    "    for target in target_types:\n",
    "        existing_keys = df[df[\"type\"] == target][\n",
    "            [\"company_name\", \"quarter\"]\n",
    "        ].drop_duplicates()\n",
    "        existing_combinations[target] = existing_keys\n",
    "\n",
    "    # Passo 2: Filtrar as linhas do source_type\n",
    "    source_df = df[df[\"type\"] == source_type].copy()\n",
    "\n",
    "    # Passo 3: Para cada target_type, verificar onde duplicar\n",
    "    duplicated_dfs = []\n",
    "    for target in target_types:\n",
    "        # Obter as combina√ß√µes onde j√° existe o target_type\n",
    "        target_keys = existing_combinations[target]\n",
    "\n",
    "        # Realizar um merge para identificar quais linhas do source_df t√™m a combina√ß√£o existente\n",
    "        to_duplicate = source_df.merge(\n",
    "            target_keys,\n",
    "            on=[\"company_name\", \"quarter\"],\n",
    "            how=\"inner\",\n",
    "            suffixes=(\"\", \"_target\"),\n",
    "        )\n",
    "\n",
    "        if not to_duplicate.empty:\n",
    "            # Duplicar as linhas e alterar o type para o target_type\n",
    "            duplicated = to_duplicate.copy()\n",
    "            duplicated[\"type\"] = target\n",
    "            duplicated_dfs.append(duplicated)\n",
    "            # print(f\"Duplicando {len(duplicated)} linhas para o tipo '{target}'.\")\n",
    "        else:\n",
    "            # print(f\"Nenhuma duplica√ß√£o necess√°ria para o tipo '{target}'.\")\n",
    "            pass\n",
    "\n",
    "    # Passo 4: Concatenar todas as duplica√ß√µes\n",
    "    if duplicated_dfs:\n",
    "        duplicated_df = pd.concat(duplicated_dfs, ignore_index=True)\n",
    "    else:\n",
    "        duplicated_df = pd.DataFrame(columns=df.columns)\n",
    "        # print(\"Nenhuma duplica√ß√£o realizada.\")\n",
    "\n",
    "    # Passo 5: Remover as linhas originais do source_type\n",
    "    df_filtered = df[df[\"type\"] != source_type].copy()\n",
    "\n",
    "    # Passo 6: Adicionar as duplica√ß√µes ao DataFrame\n",
    "    if not duplicated_df.empty:\n",
    "        df_updated = pd.concat([df_filtered, duplicated_df], ignore_index=True)\n",
    "    else:\n",
    "        df_updated = df_filtered.copy()\n",
    "\n",
    "    # Passo 7: Resetar o √≠ndice e ordenar o DataFrame (opcional)\n",
    "    df_updated.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Passo 8: Ordenar o DataFrame\n",
    "    df_updated = df_updated.sort_values(\n",
    "        by=[\n",
    "            \"sector\",\n",
    "            \"subsector\",\n",
    "            \"segment\",\n",
    "            \"company_name\",\n",
    "            \"quarter\",\n",
    "            \"version\",\n",
    "            \"type\",\n",
    "            \"account\",\n",
    "            \"description\",\n",
    "        ]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return df_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_indicators(df, frame_name, indicator_list):\n",
    "    \"\"\"\n",
    "    Calcula indicadores financeiros e os adiciona como novas linhas no DataFrame.\n",
    "\n",
    "    Par√¢metros:\n",
    "    - df (pd.DataFrame): DataFrame original contendo os dados financeiros.\n",
    "    - indicator_list (list of dict): Lista de dicion√°rios com defini√ß√µes dos indicadores.\n",
    "\n",
    "    Retorna:\n",
    "    - pd.DataFrame: DataFrame atualizado com as novas linhas de indicadores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fun√ß√£o para realizar divis√£o segura\n",
    "    def safe_division(numerator, denominator):\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            result = numerator / denominator\n",
    "            result = result.replace(\n",
    "                [np.inf, -np.inf], np.nan\n",
    "            )  # Substitui infinitos por NaN\n",
    "            return result\n",
    "\n",
    "    # Passo 1: Pivotar o DataFrame com contas como colunas e incluir 'quarter' no √≠ndice\n",
    "    pivot_df = df.pivot_table(\n",
    "        index=[\"company_name\", \"type\", \"quarter\"],\n",
    "        columns=\"account\",\n",
    "        values=\"value\",\n",
    "        aggfunc=\"sum\",\n",
    "        fill_value=0,  # Substitui valores ausentes por 0\n",
    "    ).reset_index()\n",
    "\n",
    "    # print(\"Colunas do DataFrame Pivotado:\", pivot_df.columns.tolist())\n",
    "\n",
    "    # Passo 2: Calcular os Indicadores\n",
    "    for indicator in indicator_list:\n",
    "        column_name = indicator[\"description\"]\n",
    "        column_account = indicator[\"account\"]\n",
    "        try:\n",
    "            # Aplicar a f√≥rmula para calcular o indicador\n",
    "            pivot_df[column_name] = indicator[\"formula\"](pivot_df)\n",
    "            # print(f\"Indicador '{column_name}' calculado com sucesso.\")\n",
    "        except KeyError as e:\n",
    "            print(f\"'{column_account} - {column_name}': a conta {e} n√£o existe\")\n",
    "            pivot_df[column_name] = np.nan  # Atribuir NaN se a coluna n√£o existir\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"{column_name}': {e} Erro ao calcular o indicador '{column_name}': {e}\"\n",
    "            )\n",
    "            pivot_df[column_name] = np.nan  # Atribuir NaN em caso de erro\n",
    "\n",
    "    # Passo 3: Criar Novas Linhas para os Indicadores\n",
    "    new_rows = []\n",
    "\n",
    "    for indicator in indicator_list:\n",
    "        account = indicator[\"account\"]\n",
    "        description = indicator[\"description\"]\n",
    "\n",
    "        if description not in pivot_df.columns:\n",
    "            print(\n",
    "                f\"Aviso: Indicador '{description}' n√£o foi calculado e ser√° ignorado.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Extrair os valores calculados do indicador\n",
    "        pivot_df[\"value\"] = pivot_df[description]\n",
    "\n",
    "        # Selecionar as colunas relevantes\n",
    "        indicator_values = pivot_df[[\"company_name\", \"type\", \"quarter\", \"value\"]].copy()\n",
    "        indicator_values[\"account\"] = account\n",
    "        indicator_values[\"description\"] = description\n",
    "\n",
    "        # Atribuir 'frame' como 'Indicadores' para identificar facilmente as novas linhas\n",
    "        indicator_values[\"frame\"] = frame_name\n",
    "\n",
    "        # Preencher as colunas faltantes com informa√ß√µes do DataFrame original\n",
    "        # Selecionar combina√ß√µes √∫nicas de 'company_name', 'type' e 'quarter'\n",
    "        metadata = df[\n",
    "            [\n",
    "                \"company_name\",\n",
    "                \"type\",\n",
    "                \"quarter\",\n",
    "                \"nsd\",\n",
    "                \"sector\",\n",
    "                \"subsector\",\n",
    "                \"segment\",\n",
    "                \"version\",\n",
    "            ]\n",
    "        ].drop_duplicates(subset=[\"company_name\", \"type\", \"quarter\"], keep=\"first\")\n",
    "\n",
    "        # Mesclar com 'metadata' para preencher 'nsd', 'sector', etc.\n",
    "        indicator_row = indicator_values.merge(\n",
    "            metadata, on=[\"company_name\", \"type\", \"quarter\"], how=\"left\"\n",
    "        )\n",
    "\n",
    "        # Reordenar as colunas para corresponder ao DataFrame original\n",
    "        indicator_row = indicator_row[\n",
    "            [\n",
    "                \"nsd\",\n",
    "                \"sector\",\n",
    "                \"subsector\",\n",
    "                \"segment\",\n",
    "                \"company_name\",\n",
    "                \"quarter\",\n",
    "                \"version\",\n",
    "                \"type\",\n",
    "                \"frame\",\n",
    "                \"account\",\n",
    "                \"description\",\n",
    "                \"value\",\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        # Adicionar √† lista de novas linhas\n",
    "        new_rows.append(indicator_row)\n",
    "\n",
    "    # Combinar todas as novas linhas de indicadores\n",
    "    if new_rows:\n",
    "        new_rows_df = pd.concat(new_rows, ignore_index=True)\n",
    "    else:\n",
    "        new_rows_df = pd.DataFrame(columns=df.columns)\n",
    "        # print(\"Nenhum indicador foi adicionado.\")\n",
    "\n",
    "    # Passo 4: Adicionar as Novas Linhas ao DataFrame Original\n",
    "    updated_df = pd.concat([df, new_rows_df], ignore_index=True)\n",
    "\n",
    "    # Passo 5: Tratamento Final dos Valores\n",
    "    # Opcional: Preencher valores faltantes ou realizar outras limpezas\n",
    "    # Exemplo: Substituir NaN por zero onde apropriado\n",
    "    # updated_df['value'] = updated_df['value'].fillna(0)\n",
    "\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'settings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[139], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m sector \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOMUNICACOES\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m db_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43msettings\u001b[49m\u001b[38;5;241m.\u001b[39mdb_filepath\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msettings\u001b[38;5;241m.\u001b[39mstatements_standard\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.db\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m conn \u001b[38;5;241m=\u001b[39m sqlite3\u001b[38;5;241m.\u001b[39mconnect(db_file)\n\u001b[0;32m      5\u001b[0m cursor \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'settings' is not defined"
     ]
    }
   ],
   "source": [
    "sector = \"COMUNICACOES\"\n",
    "db_file = (\n",
    "    f\"..\\data\\{settings.db_filepath.split('.')[0]} {settings.statements_standard}.db\"\n",
    ")\n",
    "\n",
    "conn = sqlite3.connect(db_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Fetch all table names excluding internal SQLite tables\n",
    "cursor.execute(\n",
    "    \"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';\"\n",
    ")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "df = pd.read_sql_query(f\"SELECT * FROM {sector}\", conn)\n",
    "# df['account_description'] = df['account'] + ' - ' + df['description']\n",
    "df[\"quarter\"] = pd.to_datetime(df[\"quarter\"])\n",
    "\n",
    "# df = clean_dados_da_empresa(df)\n",
    "df = duplicate_dfs_types(df)\n",
    "\n",
    "# Store unique values\n",
    "uv = {}\n",
    "for col in df.columns:\n",
    "    if col not in [\"nsd\", \"value\"]:\n",
    "        uv[col] = sorted(df[col].unique().tolist())\n",
    "\n",
    "df[[\"account\", \"description\"]].drop_duplicates().to_csv(\n",
    "    \"account-description.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of indicator formulas\n",
    "indicators_08 = [\n",
    "    {\n",
    "        \"account\": \"08.01\",\n",
    "        \"description\": \"ROE Return On Equity\",\n",
    "        \"formula\": lambda df: df[\"03.11\"]\n",
    "        / df[\"02.03\"]\n",
    "        * 10000,  # ROE = Net Profit / Shareholders' Equity * 100\n",
    "    },\n",
    "    {\n",
    "        \"account\": \"08.02\",\n",
    "        \"description\": \"√çndice de Liquidez Corrente\",\n",
    "        \"formula\": lambda df: df[\"01.01\"]\n",
    "        / df[\"02.01\"],  # Current Ratio = Current Assets / Current Liabilities\n",
    "    },\n",
    "    # Add more indicators as needed\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_indicators(df, \"Indicadores Fundamentalistas\", indicators_08)\n",
    "# Store unique values\n",
    "uvi = {}\n",
    "for col in df.columns:\n",
    "    if col not in [\"nsd\", \"value\"]:\n",
    "        uvi[col] = sorted(df[col].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df[\"company_name\"] == uvi[\"company_name\"][7]\n",
    "mask &= df[\"type\"] == uvi[\"type\"][1]\n",
    "mask &= df[\"account\"] == \"08.02\"\n",
    "# mask &= df2['quarter'] == '2020-12-31'\n",
    "df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols = [col for col in df2.columns if col not in [\"quarter\", \"value\"]]\n",
    "\n",
    "# Verificar duplicatas\n",
    "duplicatas = df2.duplicated(subset=index_cols + [\"quarter\"], keep=False)\n",
    "num_duplicatas = duplicatas.sum()\n",
    "\n",
    "if num_duplicatas > 0:\n",
    "    print(\"Agregando duplicatas somando os valores.\")\n",
    "    df2_agregado = df2.groupby(index_cols + [\"quarter\"], as_index=False)[\"value\"].sum()\n",
    "    # df2_agregado = df2.groupby(index_cols + ['quarter'], as_index=False)['value'].mean() # opcionalmente calcular a m√©dia\n",
    "\n",
    "else:\n",
    "    df2_agregado = df2.copy()\n",
    "\n",
    "# Pivotar o DataFrame\n",
    "df_pivot = df2_agregado.pivot_table(\n",
    "    index=index_cols,\n",
    "    columns=\"quarter\",\n",
    "    values=\"value\",\n",
    "    aggfunc=\"first\",  # 'first' assume que n√£o h√° duplicatas ap√≥s a agrega√ß√£o\n",
    ").reset_index()\n",
    "\n",
    "# # Obter uma lista dos quarters √∫nicos, ordenados de forma decrescente\n",
    "# sorted_quarters = sorted(df_pivot.columns[ len(index_cols): ], reverse=True)\n",
    "\n",
    "# # Reordenar as colunas: primeiro os √≠ndices, depois os quarters em ordem decrescente\n",
    "# df_pivot = df_pivot[index_cols + sorted_quarters]\n",
    "\n",
    "# # Opcional: formatar os nomes das colunas de quarter para strings leg√≠veis\n",
    "# df_pivot.columns = list(index_cols) + [q.strftime('%Y-%m-%d') for q in sorted_quarters]\n",
    "\n",
    "df_pivot.fillna(0, inplace=True)\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot.iloc[0:1,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector = \"COMUNICACOES\"\n",
    "db_file = (\n",
    "    f\"..\\data\\{settings.db_filepath.split('.')[0]} {settings.statements_standard}.db\"\n",
    ")\n",
    "\n",
    "conn = sqlite3.connect(db_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Fetch all table names excluding internal SQLite tables\n",
    "cursor.execute(\n",
    "    \"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';\"\n",
    ")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "df = pd.read_sql_query(f\"SELECT * FROM {sector}\", conn)\n",
    "# df['account_description'] = df['account'] + ' - ' + df['description']\n",
    "df[\"quarter\"] = pd.to_datetime(df[\"quarter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sector': ['COMUNICACOES'],\n",
       " 'subsector': ['MIDIA', 'TELECOMUNICACOES'],\n",
       " 'segment': ['PUBLICIDADE E PROPAGANDA', 'TELECOMUNICACOES'],\n",
       " 'company_name': ['ALGAR TELECOM SA',\n",
       "  'BRISANET PARTICIPACOES SA',\n",
       "  'DESKTOP SA',\n",
       "  'ELETROMIDIA SA',\n",
       "  'GIGA MAIS FIBRA TELECOMUNICACOES SA',\n",
       "  'OI SA',\n",
       "  'TELEC BRASILEIRAS SA TELEBRAS',\n",
       "  'TELEFONICA BRASIL SA',\n",
       "  'TIM SA',\n",
       "  'UNIFIQUE TELECOMUNICACOES SA'],\n",
       " 'quarter': [Timestamp('2010-12-31 00:00:00'),\n",
       "  Timestamp('2011-03-31 00:00:00'),\n",
       "  Timestamp('2011-06-30 00:00:00'),\n",
       "  Timestamp('2011-09-30 00:00:00'),\n",
       "  Timestamp('2011-12-31 00:00:00'),\n",
       "  Timestamp('2012-03-31 00:00:00'),\n",
       "  Timestamp('2012-06-30 00:00:00'),\n",
       "  Timestamp('2012-09-30 00:00:00'),\n",
       "  Timestamp('2012-12-31 00:00:00'),\n",
       "  Timestamp('2013-03-31 00:00:00'),\n",
       "  Timestamp('2013-06-30 00:00:00'),\n",
       "  Timestamp('2013-09-30 00:00:00'),\n",
       "  Timestamp('2013-12-31 00:00:00'),\n",
       "  Timestamp('2014-03-31 00:00:00'),\n",
       "  Timestamp('2014-06-30 00:00:00'),\n",
       "  Timestamp('2014-09-30 00:00:00'),\n",
       "  Timestamp('2014-12-31 00:00:00'),\n",
       "  Timestamp('2015-03-31 00:00:00'),\n",
       "  Timestamp('2015-06-30 00:00:00'),\n",
       "  Timestamp('2015-09-30 00:00:00'),\n",
       "  Timestamp('2015-12-31 00:00:00'),\n",
       "  Timestamp('2016-03-31 00:00:00'),\n",
       "  Timestamp('2016-06-30 00:00:00'),\n",
       "  Timestamp('2016-09-30 00:00:00'),\n",
       "  Timestamp('2016-12-31 00:00:00'),\n",
       "  Timestamp('2017-03-31 00:00:00'),\n",
       "  Timestamp('2017-06-30 00:00:00'),\n",
       "  Timestamp('2017-09-30 00:00:00'),\n",
       "  Timestamp('2017-12-31 00:00:00'),\n",
       "  Timestamp('2018-03-31 00:00:00'),\n",
       "  Timestamp('2018-06-30 00:00:00'),\n",
       "  Timestamp('2018-09-30 00:00:00'),\n",
       "  Timestamp('2018-12-31 00:00:00'),\n",
       "  Timestamp('2019-03-31 00:00:00'),\n",
       "  Timestamp('2019-06-30 00:00:00'),\n",
       "  Timestamp('2019-09-30 00:00:00'),\n",
       "  Timestamp('2019-12-31 00:00:00'),\n",
       "  Timestamp('2020-03-31 00:00:00'),\n",
       "  Timestamp('2020-06-30 00:00:00'),\n",
       "  Timestamp('2020-09-30 00:00:00'),\n",
       "  Timestamp('2020-12-31 00:00:00'),\n",
       "  Timestamp('2021-03-31 00:00:00'),\n",
       "  Timestamp('2021-06-30 00:00:00'),\n",
       "  Timestamp('2021-09-30 00:00:00'),\n",
       "  Timestamp('2021-12-31 00:00:00'),\n",
       "  Timestamp('2022-03-31 00:00:00'),\n",
       "  Timestamp('2022-06-30 00:00:00'),\n",
       "  Timestamp('2022-09-30 00:00:00'),\n",
       "  Timestamp('2022-12-31 00:00:00'),\n",
       "  Timestamp('2023-03-31 00:00:00'),\n",
       "  Timestamp('2023-06-30 00:00:00'),\n",
       "  Timestamp('2023-09-30 00:00:00'),\n",
       "  Timestamp('2023-12-31 00:00:00'),\n",
       "  Timestamp('2024-03-31 00:00:00'),\n",
       "  Timestamp('2024-06-30 00:00:00')],\n",
       " 'version': ['1', '2', '3', '4'],\n",
       " 'type': ['DFs Consolidadas', 'DFs Individuais', 'Dados da Empresa'],\n",
       " 'frame': ['Balan√ßo Patrimonial Ativo',\n",
       "  'Balan√ßo Patrimonial Passivo',\n",
       "  'Composi√ß√£o do Capital',\n",
       "  'Demonstra√ß√£o de Valor Adicionado',\n",
       "  'Demonstra√ß√£o do Fluxo de Caixa',\n",
       "  'Demonstra√ß√£o do Resultado'],\n",
       " 'account': ['00.01.01',\n",
       "  '00.01.02',\n",
       "  '00.02.01',\n",
       "  '00.02.02',\n",
       "  '01',\n",
       "  '01.01',\n",
       "  '01.01.01',\n",
       "  '01.01.01.01',\n",
       "  '01.01.01.02',\n",
       "  '01.01.02',\n",
       "  '01.01.02.01',\n",
       "  '01.01.02.02',\n",
       "  '01.01.03',\n",
       "  '01.01.03.01',\n",
       "  '01.01.03.01.01',\n",
       "  '01.01.03.01.02',\n",
       "  '01.01.03.01.03',\n",
       "  '01.01.03.02',\n",
       "  '01.01.04',\n",
       "  '01.01.04.01',\n",
       "  '01.01.04.02',\n",
       "  '01.01.04.03',\n",
       "  '01.01.05',\n",
       "  '01.01.06',\n",
       "  '01.01.07',\n",
       "  '01.01.09',\n",
       "  '01.02',\n",
       "  '01.02.01',\n",
       "  '01.02.01.02',\n",
       "  '01.02.01.03',\n",
       "  '01.02.01.04',\n",
       "  '01.02.01.05',\n",
       "  '01.02.01.06',\n",
       "  '01.02.01.07',\n",
       "  '01.02.01.08',\n",
       "  '01.02.01.09',\n",
       "  '01.02.02',\n",
       "  '01.02.02.01',\n",
       "  '01.02.02.01.01',\n",
       "  '01.02.02.01.02',\n",
       "  '01.02.02.01.03',\n",
       "  '01.02.02.02',\n",
       "  '01.02.03',\n",
       "  '01.02.03.01',\n",
       "  '01.02.03.02',\n",
       "  '01.02.04',\n",
       "  '01.02.04.01',\n",
       "  '01.02.04.01.01',\n",
       "  '01.02.04.01.02',\n",
       "  '01.02.04.01.03',\n",
       "  '01.02.04.02',\n",
       "  '01.02.04.02.01',\n",
       "  '02',\n",
       "  '02.01',\n",
       "  '02.01.01.01',\n",
       "  '02.01.01.02',\n",
       "  '02.01.02',\n",
       "  '02.01.02.01',\n",
       "  '02.01.02.02',\n",
       "  '02.01.03',\n",
       "  '02.01.03.01',\n",
       "  '02.01.03.01.01',\n",
       "  '02.01.03.01.02',\n",
       "  '02.01.03.01.03',\n",
       "  '02.01.03.02',\n",
       "  '02.01.03.03',\n",
       "  '02.01.04',\n",
       "  '02.01.04.01',\n",
       "  '02.01.04.01.01',\n",
       "  '02.01.04.01.02',\n",
       "  '02.01.04.02',\n",
       "  '02.01.04.03',\n",
       "  '02.01.05',\n",
       "  '02.01.05.01',\n",
       "  '02.01.05.01.01',\n",
       "  '02.01.05.01.03',\n",
       "  '02.01.05.01.04',\n",
       "  '02.01.05.02',\n",
       "  '02.01.05.02.01',\n",
       "  '02.01.05.02.02',\n",
       "  '02.01.05.02.03',\n",
       "  '02.01.05.02.04',\n",
       "  '02.01.05.02.09',\n",
       "  '02.01.06',\n",
       "  '02.01.06.01',\n",
       "  '02.01.06.01.01',\n",
       "  '02.01.06.01.02',\n",
       "  '02.01.06.01.03',\n",
       "  '02.01.06.01.04',\n",
       "  '02.01.06.02',\n",
       "  '02.01.06.02.01',\n",
       "  '02.01.06.02.02',\n",
       "  '02.01.06.02.03',\n",
       "  '02.02',\n",
       "  '02.02.01',\n",
       "  '02.02.01.01',\n",
       "  '02.02.01.01.01',\n",
       "  '02.02.01.01.02',\n",
       "  '02.02.01.02',\n",
       "  '02.02.01.03',\n",
       "  '02.02.02',\n",
       "  '02.02.02.01.04',\n",
       "  '02.02.03',\n",
       "  '02.02.04',\n",
       "  '02.02.04.01',\n",
       "  '02.03',\n",
       "  '02.03.01',\n",
       "  '02.03.01.01',\n",
       "  '02.03.01.02',\n",
       "  '02.03.02',\n",
       "  '02.03.02.01',\n",
       "  '02.03.02.02',\n",
       "  '02.03.02.09',\n",
       "  '02.03.03',\n",
       "  '02.03.04',\n",
       "  '02.03.04.01',\n",
       "  '02.03.04.02',\n",
       "  '02.03.04.03',\n",
       "  '02.03.04.09',\n",
       "  '02.03.05',\n",
       "  '02.03.06',\n",
       "  '02.03.06.01',\n",
       "  '02.03.06.02',\n",
       "  '02.03.06.09',\n",
       "  '02.03.07',\n",
       "  '02.03.08',\n",
       "  '02.03.09',\n",
       "  '03.01',\n",
       "  '03.02',\n",
       "  '03.03',\n",
       "  '03.04',\n",
       "  '03.04.01',\n",
       "  '03.04.01.01',\n",
       "  '03.05',\n",
       "  '03.06',\n",
       "  '03.07',\n",
       "  '03.08',\n",
       "  '03.11',\n",
       "  '03.11.01',\n",
       "  '03.11.02',\n",
       "  '06.01',\n",
       "  '06.02',\n",
       "  '06.03',\n",
       "  '07.01',\n",
       "  '07.01.01',\n",
       "  '07.01.02',\n",
       "  '07.01.03',\n",
       "  '07.01.04',\n",
       "  '07.02',\n",
       "  '07.02.01',\n",
       "  '07.02.02',\n",
       "  '07.02.03',\n",
       "  '07.02.04',\n",
       "  '07.03',\n",
       "  '07.04',\n",
       "  '07.04.01',\n",
       "  '07.04.02',\n",
       "  '07.05',\n",
       "  '07.06',\n",
       "  '07.06.01',\n",
       "  '07.06.02',\n",
       "  '07.06.03',\n",
       "  '07.06.03.01',\n",
       "  '07.06.03.02',\n",
       "  '07.07',\n",
       "  '07.08',\n",
       "  '07.08.01',\n",
       "  '07.08.01.01',\n",
       "  '07.08.01.02',\n",
       "  '07.08.01.03',\n",
       "  '07.08.01.04',\n",
       "  '07.08.02',\n",
       "  '07.08.02.01',\n",
       "  '07.08.02.02',\n",
       "  '07.08.02.03',\n",
       "  '07.08.03',\n",
       "  '07.08.03.01',\n",
       "  '07.08.03.02',\n",
       "  '07.08.03.03',\n",
       "  '07.08.04',\n",
       "  '07.08.04.01',\n",
       "  '07.08.04.02',\n",
       "  '07.08.04.03',\n",
       "  '07.08.04.04',\n",
       "  '07.08.05.01',\n",
       "  '07.08.05.02',\n",
       "  '07.08.05.03',\n",
       "  '07.08.05.04',\n",
       "  '07.08.05.09'],\n",
       " 'description': ['Ajustes Acumulados de Convers√£o',\n",
       "  'Ajustes Patrimoniais',\n",
       "  'Ajustes de Avalia√ß√£o Patrimonial',\n",
       "  'Alugu√©is',\n",
       "  'Aplica√ß√µes Financeiras de Curto Prazo',\n",
       "  'Aplica√ß√µes L√≠quidas de Curto Prazo',\n",
       "  'Aplica√ß√µes a Valor Justo de Curto Prazo',\n",
       "  'Aplica√ß√µes a Valor Justo de Longo Prazo',\n",
       "  'Aplica√ß√µes ao Custo Amortizado de Curto Prazo',\n",
       "  'Ativo Circulante de Curto Prazo',\n",
       "  'Ativo N√£o Circulante de Longo Prazo',\n",
       "  'Ativo Realiz√°vel a Longo Prazo',\n",
       "  'Ativo Total',\n",
       "  'Ativos Biol√≥gicos de Curto Prazo',\n",
       "  'Ativos Biol√≥gicos de Longo Prazo',\n",
       "  'Atribu√≠do a S√≥cios N√£o Controladores',\n",
       "  'Atribu√≠do a S√≥cios da Empresa Controladora',\n",
       "  'A√ß√µes ON Ordin√°rias',\n",
       "  'A√ß√µes PN Preferenciais',\n",
       "  'A√ß√µes, Remunera√ß√£o e Op√ß√µes',\n",
       "  'Benef√≠cios',\n",
       "  'Caixa de Financiamento',\n",
       "  'Caixa de Investimento',\n",
       "  'Caixa de Opera√ß√µes (Operacional)',\n",
       "  'Caixa e Bancos de Curto Prazo',\n",
       "  'Caixa e Equivalentes de Caixa de Curto Prazo',\n",
       "  'Capital Social',\n",
       "  'Capital Social Realizado',\n",
       "  'Carteira de Clientes',\n",
       "  'Clientes',\n",
       "  'Contas a Receber de Curto Prazo',\n",
       "  'Contas a Receber de Longo Prazo',\n",
       "  'Contas de Clientes de Curto Prazo',\n",
       "  'Cr√©ditos com Partes Relacionadas de Longo Prazo',\n",
       "  'Cr√©ditos de Liquida√ß√£o Duvidosa',\n",
       "  'Custo dos Bens e/ou Servi√ßos Vendidos',\n",
       "  'Custos Prods., Mercs. e Servs. Vendidos',\n",
       "  'Deb√™ntures',\n",
       "  'Deprecia√ß√£o, Amortiza√ß√£o e Exaust√£o',\n",
       "  'Derivativos e Participa√ß√µes',\n",
       "  'Despesas Antecipadas de Curto Prazo',\n",
       "  'Despesas Antecipadas de Longo Prazo',\n",
       "  'Despesas Comerciais',\n",
       "  'Despesas Operacionais',\n",
       "  'Despesas/Receitas Operacionais',\n",
       "  'Direito de Uso em Arrendamento',\n",
       "  'Distribui√ß√£o do Valor Adicionado',\n",
       "  'Dividendos',\n",
       "  'Dividendos e A√ß√µes',\n",
       "  'Dividendos e A√ß√µes em Tesouraria',\n",
       "  'D√©bitos com Coligadas',\n",
       "  'D√©bitos com Controladores',\n",
       "  'D√©bitos com Outras Partes Relacionadas',\n",
       "  'Em Moeda Estrangeira',\n",
       "  'Em Moeda Nacional',\n",
       "  'Em Tesouraria A√ß√µes ON Ordin√°rias',\n",
       "  'Em Tesouraria A√ß√µes PN Preferenciais',\n",
       "  'Empr√©stimos e Financiamentos',\n",
       "  'Empr√©stimos e Financiamentos de Curto Prazo',\n",
       "  'Empr√©stimos e Financiamentos de Longo Prazo',\n",
       "  'Estaduais',\n",
       "  'Estoques de Curto Prazo',\n",
       "  'Estoques de Longo Prazo',\n",
       "  'Estoques de Material de Consumo de Curto Prazo',\n",
       "  'Estoques de Material para Revenda de Curto Prazo',\n",
       "  'Estoques de Outros Itens de Curto Prazo',\n",
       "  'F.G.T.S.',\n",
       "  'Federais',\n",
       "  'Financiamento por Arrendamento Financeiro',\n",
       "  'Fornecedores Estrangeiros',\n",
       "  'Fornecedores Nacionais',\n",
       "  'Fornecedores de Curto Prazo',\n",
       "  'Gastos na emiss√£o de a√ß√µes',\n",
       "  'Goodwill',\n",
       "  'Imobilizado',\n",
       "  'Imobilizado em Opera√ß√£o',\n",
       "  'Imposto de Renda e Contribui√ß√£o Social Diferidos',\n",
       "  'Imposto de Renda e Contribui√ß√£o Social a Pagar',\n",
       "  'Imposto de Renda e Contribui√ß√£o Social sobre o Lucro',\n",
       "  'Impostos, Taxas e Contribui√ß√µes',\n",
       "  'Insumos Adquiridos de Terceiros',\n",
       "  'Intang√≠veis',\n",
       "  'Intang√≠vel',\n",
       "  'Investimento Social',\n",
       "  'Investimentos',\n",
       "  'Juros',\n",
       "  'Juros sobre o Capital Pr√≥prio',\n",
       "  'Lucro do Per√≠odo',\n",
       "  'Lucros Retidos',\n",
       "  'Lucros Retidos / Preju√≠zo do Per√≠odo',\n",
       "  'Lucros/Preju√≠zos Acumulados',\n",
       "  'Marcas e Patentes',\n",
       "  'Materiais, Energia, Servs. de Terceiros e Outros',\n",
       "  'Municipais',\n",
       "  'Obriga√ß√µes Fiscais Estaduais',\n",
       "  'Obriga√ß√µes Fiscais Federais',\n",
       "  'Obriga√ß√µes Fiscais Municipais',\n",
       "  'Obriga√ß√µes Fiscais de Curto Prazo',\n",
       "  'Obriga√ß√µes Sociais',\n",
       "  'Obriga√ß√µes Trabalhistas',\n",
       "  'Obriga√ß√µes Tribut√°rias e Autoriza√ß√µes',\n",
       "  'Outras',\n",
       "  'Outras Contas de Curto Prazo',\n",
       "  'Outras Obriga√ß√µes Fiscais Federais',\n",
       "  'Outras Obriga√ß√µes de Curto Prazo',\n",
       "  'Outras Provis√µes',\n",
       "  'Outras Receitas',\n",
       "  'Outros',\n",
       "  'Outros Ativos Circulantes de Curto Prazo',\n",
       "  'Outros Ativos Circulantes de Longo Prazo',\n",
       "  'Outros Resultados Abrangentes',\n",
       "  'Part. N√£o Controladores nos Lucros Retidos',\n",
       "  'Participa√ß√£o Minorit√°ria',\n",
       "  'Participa√ß√£o dos Acionistas N√£o Controladores',\n",
       "  'Participa√ß√µes Societ√°rias',\n",
       "  'Participa√ß√µes em Coligadas',\n",
       "  'Participa√ß√µes em Controladas',\n",
       "  'Passivo Circulante de Curto Prazo',\n",
       "  'Passivo N√£o Circulante de Longo Prazo',\n",
       "  'Passivo Total',\n",
       "  'Passivos com Partes Relacionadas',\n",
       "  'Passivos com Partes Relacionadas de Longo Prazo',\n",
       "  'Patrim√¥nio L√≠quido',\n",
       "  'Perda/Recupera√ß√£o de Valores Ativos',\n",
       "  'Perdas e Aquisi√ß√µes com N√£o Controladores',\n",
       "  'Pessoal',\n",
       "  'Propriedades para Investimento',\n",
       "  'Provis√£o/Revers√£o de Cr√©ds. Liquida√ß√£o Duvidosa',\n",
       "  'Provis√µes C√≠veis',\n",
       "  'Provis√µes Fiscais',\n",
       "  'Provis√µes Fiscais Previdenci√°rias Trabalhistas e C√≠veis',\n",
       "  'Provis√µes Judiciais',\n",
       "  'Provis√µes Previdenci√°rias e Trabalhistas',\n",
       "  'Provis√µes de Curto Prazo',\n",
       "  'Provis√µes de Longo Prazo',\n",
       "  'Provis√µes para Benef√≠cios a Empregados',\n",
       "  'Provis√µes para Garantias',\n",
       "  'Provis√µes para Passivos Ambientais e de Desativa√ß√£o',\n",
       "  'Provis√µes para Reestrutura√ß√£o',\n",
       "  'Provis√µes trabalhistas e c√≠veis, l√≠quidas',\n",
       "  'Receita de Venda de Bens e/ou Servi√ßos',\n",
       "  'Receitas',\n",
       "  'Receitas Financeiras',\n",
       "  'Receitas refs. √† Constru√ß√£o de Ativos Pr√≥prios',\n",
       "  'Remunera√ß√£o Direta',\n",
       "  'Remunera√ß√£o de Capitais Pr√≥prios',\n",
       "  'Remunera√ß√£o de Capitais de Terceiros',\n",
       "  'Reservas Legais e Estatut√°rias',\n",
       "  'Reservas de Capital',\n",
       "  'Reservas de Lucros',\n",
       "  'Reservas de Reavalia√ß√£o',\n",
       "  'Resultado Antes do Resultado Financeiro e dos Tributos',\n",
       "  'Resultado Antes dos Tributos sobre o Lucro',\n",
       "  'Resultado Bruto',\n",
       "  'Resultado Financeiro',\n",
       "  'Resultado de Equival√™ncia Patrimonial',\n",
       "  'Reten√ß√£o de Lucros e Incentivos Fiscais',\n",
       "  'Reten√ß√µes',\n",
       "  'Softwares',\n",
       "  'Telecomunica√ß√µes e Consigna√ß√µes',\n",
       "  'Tributos Parcelados',\n",
       "  'Tributos a Recuperar de Curto Prazo',\n",
       "  'Tributos a Recuperar de Longo Prazo',\n",
       "  'Valor Adicionado Bruto',\n",
       "  'Valor Adicionado L√≠quido Produzido',\n",
       "  'Valor Adicionado Total a Distribuir',\n",
       "  'Vendas de Mercadorias, Produtos e Servi√ßos',\n",
       "  'Vlr Adicionado Recebido em Transfer√™ncia',\n",
       "  '√Ågio e Reserva Especial']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store unique values\n",
    "uv = {}\n",
    "for col in df.columns:\n",
    "    if col not in [\"nsd\", \"value\"]:\n",
    "        uv[col] = sorted(df[col].unique().tolist())\n",
    "uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_series(\n",
    "    company_name,\n",
    "    account,\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    window=4,\n",
    "    std_dev=2,\n",
    "    normalize=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot a time series of a financial metric for a given company, with Bollinger Bands and custom blue color scheme.\n",
    "\n",
    "    Args:\n",
    "    - company_name: Name of the company.\n",
    "    - account: The financial account to be plotted.\n",
    "    - start_date: Run date for filtering the data (optional).\n",
    "    - end_date: End date for filtering the data (optional).\n",
    "    - window: The window size for the moving average (default is 4).\n",
    "    - std_dev: The number of standard deviations for the Bollinger Bands (default is 2).\n",
    "    \"\"\"\n",
    "    # Define blue color scheme\n",
    "    color_blue = {\n",
    "        \"main\": \"blue\",\n",
    "        \"mma\": \"lightblue\",\n",
    "        \"bands\": \"lightblue\",\n",
    "        \"fill\": \"rgba(173, 216, 230, 0.2)\",\n",
    "        \"extra_bands\": \"lightgray\",\n",
    "        \"extra_fill\": \"rgba(200, 200, 200, 0.2)\",\n",
    "    }\n",
    "\n",
    "    # Try 'DFs Consolidadas' first\n",
    "    if (df[\"type\"] == \"DFs Consolidadas\").any():\n",
    "        df_type = \"DFs Consolidadas\"\n",
    "    else:\n",
    "        df_type = \"DFs Individuais\"\n",
    "\n",
    "    # Filter data\n",
    "    data = df[\n",
    "        (df[\"company_name\"] == company_name)\n",
    "        & (df[\"account\"] == account)\n",
    "        & (df[\"type\"] == df_type)\n",
    "    ]\n",
    "\n",
    "    # Apply date filters if provided\n",
    "    if start_date:\n",
    "        data = data[data[\"quarter\"] >= pd.to_datetime(start_date)]\n",
    "    if end_date:\n",
    "        data = data[data[\"quarter\"] <= pd.to_datetime(end_date)]\n",
    "\n",
    "    if data.empty:\n",
    "        print(\"No data available for the given filters, check parameters.\")\n",
    "        return\n",
    "\n",
    "    if normalize == True:\n",
    "        # Drop the first row if its value is zero\n",
    "        if data[\"value\"].iloc[0] == 0:\n",
    "            data = data.iloc[1:].copy()\n",
    "\n",
    "        # Get the first non-zero value\n",
    "        first_non_zero = data[\"value\"].iloc[0]\n",
    "\n",
    "        # Normalize the values by dividing by the first non-zero value\n",
    "        data[\"value\"] = data[\"value\"] / first_non_zero\n",
    "\n",
    "        # Scale the values so that the maximum is 100\n",
    "        max_value = data[\"value\"].max()\n",
    "        data[\"value\"] = (data[\"value\"] / max_value) * 100\n",
    "\n",
    "    # Sort data by date\n",
    "    data = data.sort_values(\"quarter\")\n",
    "\n",
    "    # Calculate the moving average (Middle Band)\n",
    "    data[\"mma\"] = data[\"value\"].rolling(window=window).mean()\n",
    "\n",
    "    # Calculate the rolling standard deviation\n",
    "    data[\"std_dev\"] = data[\"value\"].rolling(window=window).std()\n",
    "\n",
    "    # Calculate upper and lower Bollinger Bands with std_dev multiplier\n",
    "    data[\"upper_band\"] = data[\"mma\"] + std_dev * data[\"std_dev\"]\n",
    "    data[\"lower_band\"] = data[\"mma\"] - std_dev * data[\"std_dev\"]\n",
    "\n",
    "    # Calculate extra upper and lower bands using std_dev * 1.5\n",
    "    extra_std_dev = std_dev * 1.5\n",
    "    data[\"upper_band_extra\"] = data[\"mma\"] + extra_std_dev * data[\"std_dev\"]\n",
    "    data[\"lower_band_extra\"] = data[\"mma\"] - extra_std_dev * data[\"std_dev\"]\n",
    "\n",
    "    # Set description to the first non-null description value in the filtered data\n",
    "    description = (\n",
    "        data[\"description\"].dropna().iloc[0]\n",
    "        if \"description\" in data.columns and not data[\"description\"].isna().all()\n",
    "        else account\n",
    "    )\n",
    "\n",
    "    # Create line plot with original values\n",
    "    fig = px.line(\n",
    "        data,\n",
    "        x=\"quarter\",\n",
    "        y=\"value\",\n",
    "        title=f\"{account} - {description} with Bollinger Bands for {company_name} ({df_type})\",\n",
    "        labels={\"quarter\": \"Quarter\", \"value\": description},\n",
    "        template=\"plotly_white\",\n",
    "    )\n",
    "\n",
    "    # Add main line for the data\n",
    "    fig.update_traces(\n",
    "        line=dict(color=color_blue[\"main\"]), name=\"Original Data\", showlegend=False\n",
    "    )\n",
    "\n",
    "    # Add moving average (MMA) as a dashed light blue line\n",
    "    fig.add_scatter(\n",
    "        x=data[\"quarter\"],\n",
    "        y=data[\"mma\"],\n",
    "        mode=\"lines\",\n",
    "        name=f\"Middle Band (window={window})\",\n",
    "        line=dict(color=color_blue[\"mma\"], dash=\"dash\"),\n",
    "        showlegend=False,\n",
    "    )\n",
    "\n",
    "    # Add upper and lower Bollinger Bands (std_dev * 2) and group them\n",
    "    fig.add_scatter(\n",
    "        x=data[\"quarter\"],\n",
    "        y=data[\"upper_band\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"Upper Band (std_dev * 2)\",\n",
    "        line=dict(dash=\"solid\", color=color_blue[\"bands\"]),\n",
    "        legendgroup=\"Bands (std_dev * 2)\",\n",
    "        showlegend=False,\n",
    "    )\n",
    "\n",
    "    fig.add_scatter(\n",
    "        x=data[\"quarter\"],\n",
    "        y=data[\"lower_band\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"Lower Band (std_dev * 2)\",\n",
    "        line=dict(dash=\"solid\", color=color_blue[\"bands\"]),\n",
    "        legendgroup=\"Bands (std_dev * 2)\",\n",
    "        showlegend=False,\n",
    "    )\n",
    "\n",
    "    # Fill the area between the upper and lower bands (std_dev * 2)\n",
    "    fig.add_scatter(\n",
    "        x=data[\"quarter\"],\n",
    "        y=data[\"upper_band\"],\n",
    "        fill=None,\n",
    "        mode=\"lines\",\n",
    "        name=\"Upper Band Fill (std_dev * 2)\",\n",
    "        line=dict(color=\"rgba(0,0,0,0)\"),\n",
    "        showlegend=False,\n",
    "        legendgroup=\"Bands (std_dev * 2)\",\n",
    "    )\n",
    "\n",
    "    fig.add_scatter(\n",
    "        x=data[\"quarter\"],\n",
    "        y=data[\"lower_band\"],\n",
    "        fill=\"tonexty\",\n",
    "        mode=\"lines\",\n",
    "        name=\"Lower Band Fill (std_dev * 2)\",\n",
    "        line=dict(color=\"rgba(0,0,0,0)\"),\n",
    "        fillcolor=color_blue[\"fill\"],\n",
    "        showlegend=False,\n",
    "        legendgroup=\"Bands (std_dev * 2)\",\n",
    "    )\n",
    "\n",
    "    # Add extra upper and lower Bollinger Bands (std_dev * 1.5) and group them\n",
    "    fig.add_scatter(\n",
    "        x=data[\"quarter\"],\n",
    "        y=data[\"upper_band_extra\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"Upper Band (std_dev * 1.5)\",\n",
    "        line=dict(dash=\"dot\", color=color_blue[\"extra_bands\"]),\n",
    "        legendgroup=\"Bands (std_dev * 1.5)\",\n",
    "        showlegend=False,\n",
    "    )\n",
    "\n",
    "    fig.add_scatter(\n",
    "        x=data[\"quarter\"],\n",
    "        y=data[\"lower_band_extra\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"Lower Band (std_dev * 1.5)\",\n",
    "        line=dict(dash=\"dot\", color=color_blue[\"extra_bands\"]),\n",
    "        legendgroup=\"Bands (std_dev * 1.5)\",\n",
    "        showlegend=False,\n",
    "    )\n",
    "\n",
    "    # Fill the area between the extra upper and lower bands (std_dev * 1.5)\n",
    "    fig.add_scatter(\n",
    "        x=data[\"quarter\"],\n",
    "        y=data[\"upper_band_extra\"],\n",
    "        fill=None,\n",
    "        mode=\"lines\",\n",
    "        name=\"Upper Band Extra Fill (std_dev * 1.5)\",\n",
    "        line=dict(color=\"rgba(0,0,0,0)\"),\n",
    "        showlegend=False,\n",
    "        legendgroup=\"Bands (std_dev * 1.5)\",\n",
    "    )\n",
    "\n",
    "    fig.add_scatter(\n",
    "        x=data[\"quarter\"],\n",
    "        y=data[\"lower_band_extra\"],\n",
    "        fill=\"tonexty\",\n",
    "        mode=\"lines\",\n",
    "        name=\"Lower Band Extra Fill (std_dev * 1.5)\",\n",
    "        line=dict(color=\"rgba(0,0,0,0)\"),\n",
    "        fillcolor=color_blue[\"extra_fill\"],\n",
    "        showlegend=False,\n",
    "        legendgroup=\"Bands (std_dev * 1.5)\",\n",
    "    )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(xaxis_title=\"Quarter\", yaxis_title=description)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = plot_series(\n",
    "    company_name=uv[\"company_name\"][7],  # Change to your desired company\n",
    "    account=\"01.01\",\n",
    "    # start_date='2015-01-01',\n",
    "    # end_date='2023-06-30',\n",
    "    window=4,\n",
    "    normalize=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series(\n",
    "    company_name, accounts, start_date=None, end_date=None, plot_type=\"stacked\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot a stacked, overlapping, line-only, 100% stacked, or normalized stacked area chart of financial metrics for multiple accounts of a given company.\n",
    "\n",
    "    Args:\n",
    "    - company_name: Name of the company.\n",
    "    - accounts: List of financial accounts to be plotted.\n",
    "    - start_date: Run date for filtering the data (optional).\n",
    "    - end_date: End date for filtering the data (optional).\n",
    "    - plot_type: Determines the type of plot: \"stacked\", \"overlapping\", \"lines\", \"100_stacked\", or \"normalized_stacked\".\n",
    "    \"\"\"\n",
    "    # Filter and combine data for all accounts\n",
    "    filtered_data = pd.DataFrame()\n",
    "\n",
    "    for account in accounts:\n",
    "        # Try 'DFs Consolidadas' first\n",
    "        if (df[\"type\"] == \"DFs Consolidadas\").any():\n",
    "            df_type = \"DFs Consolidadas\"\n",
    "        else:\n",
    "            df_type = \"DFs Individuais\"\n",
    "\n",
    "        # Filter data for the current account\n",
    "        data = df[\n",
    "            (df[\"company_name\"] == company_name)\n",
    "            & (df[\"account\"] == account)\n",
    "            & (df[\"type\"] == df_type)\n",
    "        ]\n",
    "\n",
    "        # Apply date filters if provided\n",
    "        if start_date:\n",
    "            data = data[data[\"quarter\"] >= pd.to_datetime(start_date)]\n",
    "        if end_date:\n",
    "            data = data[data[\"quarter\"] <= pd.to_datetime(end_date)]\n",
    "\n",
    "        if data.empty:\n",
    "            print(f\"No data available for account {account}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Apply normalization only when plot_type is \"normalized_stacked\"\n",
    "        if plot_type == \"normalized_stacked\":\n",
    "            # Drop the first row if its value is zero\n",
    "            if data[\"value\"].iloc[0] == 0:\n",
    "                data = data.iloc[1:].copy()\n",
    "\n",
    "            # Get the first non-zero value and normalize all data by that value\n",
    "            first_non_zero_value = data[\"value\"][data[\"value\"] != 0].iloc[0]\n",
    "\n",
    "            # Normalize to ensure the first non-zero value is 1\n",
    "            data.loc[:, \"value\"] = data[\"value\"] / first_non_zero_value\n",
    "\n",
    "        # Sort data by date\n",
    "        data = data.sort_values(\"quarter\")\n",
    "\n",
    "        # Add the account as a column for identifying each account\n",
    "        data[\"account\"] = account\n",
    "\n",
    "        # Append to the cumulative data\n",
    "        filtered_data = pd.concat([filtered_data, data])\n",
    "\n",
    "    if filtered_data.empty:\n",
    "        print(\"No data available for the given filters, check parameters.\")\n",
    "        return\n",
    "\n",
    "    # Determine the Y-axis label\n",
    "    y_axis_title = \"Value\"\n",
    "\n",
    "    # Plot logic for different plot types\n",
    "    if plot_type == \"stacked\":\n",
    "        # Stacked area plot with raw values\n",
    "        fig = px.area(\n",
    "            filtered_data,\n",
    "            x=\"quarter\",\n",
    "            y=\"value\",\n",
    "            color=\"account\",\n",
    "            title=f\"Stacked Area Plot of Accounts for {company_name}\",\n",
    "            labels={\"quarter\": \"Quarter\", \"value\": y_axis_title},\n",
    "            template=\"plotly_white\",\n",
    "        )\n",
    "\n",
    "    elif plot_type == \"overlapping\":\n",
    "        # Overlapping area plot (using filled line plots to avoid stacking)\n",
    "        fig = px.line(\n",
    "            filtered_data,\n",
    "            x=\"quarter\",\n",
    "            y=\"value\",\n",
    "            color=\"account\",\n",
    "            title=f\"Overlapping Area Plot of Accounts for {company_name}\",\n",
    "            labels={\"quarter\": \"Quarter\", \"value\": y_axis_title},\n",
    "            template=\"plotly_white\",\n",
    "        )\n",
    "\n",
    "        # Add filled areas to create the overlapping effect\n",
    "        for account in accounts:\n",
    "            account_data = filtered_data[filtered_data[\"account\"] == account]\n",
    "            fig.add_scatter(\n",
    "                x=account_data[\"quarter\"],\n",
    "                y=account_data[\"value\"],\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=0.5),\n",
    "                fill=\"tozeroy\",\n",
    "                name=account,\n",
    "                legendgroup=account,  # Group line and area under the same legend\n",
    "                showlegend=False,  # Hide the second entry for the filled area\n",
    "            )\n",
    "\n",
    "    elif plot_type == \"lines\":\n",
    "        # Line-only plot with no filling\n",
    "        fig = px.line(\n",
    "            filtered_data,\n",
    "            x=\"quarter\",\n",
    "            y=\"value\",\n",
    "            color=\"account\",\n",
    "            title=f\"Line-Only Plot of Accounts for {company_name}\",\n",
    "            labels={\"quarter\": \"Quarter\", \"value\": y_axis_title},\n",
    "            template=\"plotly_white\",\n",
    "        )\n",
    "\n",
    "    elif plot_type == \"100_stacked\":\n",
    "        # 100% stacked area plot (scaled to show percentages)\n",
    "        filtered_data[\"total_per_quarter\"] = filtered_data.groupby(\"quarter\")[\n",
    "            \"value\"\n",
    "        ].transform(\"sum\")\n",
    "        filtered_data[\"value\"] = (\n",
    "            filtered_data[\"value\"] / filtered_data[\"total_per_quarter\"]\n",
    "        ) * 100\n",
    "        y_axis_title = \"Percentage Contribution (%)\"\n",
    "\n",
    "        fig = px.area(\n",
    "            filtered_data,\n",
    "            x=\"quarter\",\n",
    "            y=\"value\",\n",
    "            color=\"account\",\n",
    "            title=f\"100% Stacked Area Plot of Accounts for {company_name}\",\n",
    "            labels={\"quarter\": \"Quarter\", \"value\": y_axis_title},\n",
    "            template=\"plotly_white\",\n",
    "        )\n",
    "\n",
    "    elif plot_type == \"normalized_stacked\":\n",
    "        # Normalized stacked area plot (no percentage, just normalized values for each account)\n",
    "        y_axis_title = \"Normalized Value\"\n",
    "\n",
    "        fig = px.area(\n",
    "            filtered_data,\n",
    "            x=\"quarter\",\n",
    "            y=\"value\",\n",
    "            color=\"account\",\n",
    "            title=f\"Normalized Stacked Area Plot of Accounts for {company_name}\",\n",
    "            labels={\"quarter\": \"Quarter\", \"value\": y_axis_title},\n",
    "            template=\"plotly_white\",\n",
    "        )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(xaxis_title=\"Quarter\", yaxis_title=y_axis_title, showlegend=True)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     - plot_type: Determines the type of plot: \"stacked\", \"lines\", \"overlapping\", \"100_stacked\", or \"normalized_stacked\".\n",
    "# Example usage: Line-only plot\n",
    "data = plot_time_series(\n",
    "    company_name=uv[\"company_name\"][7],\n",
    "    accounts=[\n",
    "        \"01.01.01\",\n",
    "        \"01.01.02\",\n",
    "        \"01.01.03\",\n",
    "        \"01.01.04\",\n",
    "    ],\n",
    "    plot_type=\"stacked\",  # Line-only plot\n",
    "    start_date=\"2013-12-31\",\n",
    ")\n",
    "\n",
    "# Example usage: Line-only plot\n",
    "data = plot_time_series(\n",
    "    company_name=uv[\"company_name\"][7],\n",
    "    accounts=[\n",
    "        \"01.01.01\",\n",
    "        \"01.01.02\",\n",
    "        \"01.01.03\",\n",
    "        \"01.01.04\",\n",
    "    ],\n",
    "    plot_type=\"100_stacked\",  # Line-only plot\n",
    "    start_date=\"2013-12-31\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series(\n",
    "    company_name, accounts, start_date=None, end_date=None, plot_type=\"stacked\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot a stacked, overlapping, line-only, 100% stacked, or normalized stacked area chart of financial metrics for multiple accounts of a given company.\n",
    "\n",
    "    Args:\n",
    "    - company_name: Name of the company.\n",
    "    - accounts: List of financial accounts to be plotted.\n",
    "    - start_date: Run date for filtering the data (optional).\n",
    "    - end_date: End date for filtering the data (optional).\n",
    "    - plot_type: Determines the type of plot: \"stacked\", \"overlapping\", \"lines\", \"100_stacked\", or \"normalized_stacked\".\n",
    "    \"\"\"\n",
    "    # Filter and combine data for all accounts\n",
    "    filtered_data = pd.DataFrame()\n",
    "\n",
    "    for account in accounts:\n",
    "        # Try 'DFs Consolidadas' first\n",
    "        if (df[\"type\"] == \"DFs Consolidadas\").any():\n",
    "            df_type = \"DFs Consolidadas\"\n",
    "        else:\n",
    "            df_type = \"DFs Individuais\"\n",
    "\n",
    "        # Filter data for the current account\n",
    "        data = df[\n",
    "            (df[\"company_name\"] == company_name)\n",
    "            & (df[\"account\"] == account)\n",
    "            & (df[\"type\"] == df_type)\n",
    "        ]\n",
    "\n",
    "        # Apply date filters if provided\n",
    "        if start_date:\n",
    "            data = data[data[\"quarter\"] >= pd.to_datetime(start_date)]\n",
    "        if end_date:\n",
    "            data = data[data[\"quarter\"] <= pd.to_datetime(end_date)]\n",
    "\n",
    "        if data.empty:\n",
    "            print(f\"No data available for account {account}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Apply normalization only when plot_type is \"normalized_stacked\"\n",
    "        if plot_type == \"normalized_stacked\":\n",
    "            # Drop the first row if its value is zero\n",
    "            if data[\"value\"].iloc[0] == 0:\n",
    "                data = data.iloc[1:].copy()\n",
    "\n",
    "            # Get the first non-zero value and normalize all data by that value\n",
    "            first_non_zero_value = data[\"value\"][data[\"value\"] != 0].iloc[0]\n",
    "\n",
    "            # Normalize to ensure the first non-zero value is 1\n",
    "            data.loc[:, \"value\"] = data[\"value\"] / first_non_zero_value\n",
    "\n",
    "        # Sort data by date\n",
    "        data = data.sort_values(\"quarter\")\n",
    "\n",
    "        # Add the account as a column for identifying each account\n",
    "        data[\"account\"] = account\n",
    "\n",
    "        # Append to the cumulative data\n",
    "        filtered_data = pd.concat([filtered_data, data])\n",
    "\n",
    "    if filtered_data.empty:\n",
    "        print(\"No data available for the given filters, check parameters.\")\n",
    "        return\n",
    "\n",
    "    # Determine the Y-axis label\n",
    "    y_axis_title = \"Value\"\n",
    "\n",
    "    # Plot logic for different plot types\n",
    "    if plot_type == \"stacked\":\n",
    "        # Stacked area plot with raw values\n",
    "        fig = px.area(\n",
    "            filtered_data,\n",
    "            x=\"quarter\",\n",
    "            y=\"value\",\n",
    "            color=\"account\",\n",
    "            title=f\"Stacked Area Plot of Accounts for {company_name}\",\n",
    "            labels={\"quarter\": \"Quarter\", \"value\": y_axis_title},\n",
    "            template=\"plotly_white\",\n",
    "        )\n",
    "\n",
    "    elif plot_type == \"overlapping\":\n",
    "        # Overlapping area plot (using filled line plots to avoid stacking)\n",
    "        fig = px.line(\n",
    "            filtered_data,\n",
    "            x=\"quarter\",\n",
    "            y=\"value\",\n",
    "            color=\"account\",\n",
    "            title=f\"Overlapping Area Plot of Accounts for {company_name}\",\n",
    "            labels={\"quarter\": \"Quarter\", \"value\": y_axis_title},\n",
    "            template=\"plotly_white\",\n",
    "        )\n",
    "\n",
    "        # Add filled areas to create the overlapping effect\n",
    "        for account in accounts:\n",
    "            account_data = filtered_data[filtered_data[\"account\"] == account]\n",
    "            fig.add_scatter(\n",
    "                x=account_data[\"quarter\"],\n",
    "                y=account_data[\"value\"],\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=0.5),\n",
    "                fill=\"tozeroy\",\n",
    "                name=account,\n",
    "                legendgroup=account,  # Group line and area under the same legend\n",
    "                showlegend=False,  # Hide the second entry for the filled area\n",
    "            )\n",
    "\n",
    "    elif plot_type == \"lines\":\n",
    "        # Line-only plot with no filling\n",
    "        fig = px.line(\n",
    "            filtered_data,\n",
    "            x=\"quarter\",\n",
    "            y=\"value\",\n",
    "            color=\"account\",\n",
    "            title=f\"Line-Only Plot of Accounts for {company_name}\",\n",
    "            labels={\"quarter\": \"Quarter\", \"value\": y_axis_title},\n",
    "            template=\"plotly_white\",\n",
    "        )\n",
    "\n",
    "    elif plot_type == \"100_stacked\":\n",
    "        # 100% stacked area plot (scaled to show percentages)\n",
    "        filtered_data[\"total_per_quarter\"] = filtered_data.groupby(\"quarter\")[\n",
    "            \"value\"\n",
    "        ].transform(\"sum\")\n",
    "        filtered_data[\"value\"] = (\n",
    "            filtered_data[\"value\"] / filtered_data[\"total_per_quarter\"]\n",
    "        ) * 100\n",
    "        y_axis_title = \"Percentage Contribution (%)\"\n",
    "\n",
    "        fig = px.area(\n",
    "            filtered_data,\n",
    "            x=\"quarter\",\n",
    "            y=\"value\",\n",
    "            color=\"account\",\n",
    "            title=f\"100% Stacked Area Plot of Accounts for {company_name}\",\n",
    "            labels={\"quarter\": \"Quarter\", \"value\": y_axis_title},\n",
    "            template=\"plotly_white\",\n",
    "        )\n",
    "\n",
    "    elif plot_type == \"normalized_stacked\":\n",
    "        # Normalized stacked area plot (no percentage, just normalized values for each account)\n",
    "        y_axis_title = \"Normalized Value\"\n",
    "\n",
    "        fig = px.area(\n",
    "            filtered_data,\n",
    "            x=\"quarter\",\n",
    "            y=\"value\",\n",
    "            color=\"account\",\n",
    "            title=f\"Normalized Stacked Area Plot of Accounts for {company_name}\",\n",
    "            labels={\"quarter\": \"Quarter\", \"value\": y_axis_title},\n",
    "            template=\"plotly_white\",\n",
    "        )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(xaxis_title=\"Quarter\", yaxis_title=y_axis_title, showlegend=True)\n",
    "\n",
    "    return fig  # Return the figure, not the DataFrame\n",
    "\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Initialize a subplot figure with 2 rows and 1 column\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1, subplot_titles=(\"Stacked Plot\", \"100% Stacked Plot\")\n",
    ")\n",
    "\n",
    "# Call plot_time_series for the stacked plot and add to the first subplot (row 1)\n",
    "fig_stacked = plot_time_series(\n",
    "    company_name=uv[\"company_name\"][7],\n",
    "    accounts=[\"01.01.01\", \"01.01.02\", \"01.01.03\", \"01.01.04\"],\n",
    "    plot_type=\"stacked\",\n",
    "    start_date=\"2013-12-31\",\n",
    ")\n",
    "\n",
    "# Add the traces from the stacked plot to the first subplot\n",
    "for trace in fig_stacked[\"data\"]:\n",
    "    fig.add_trace(trace, row=1, col=1)\n",
    "\n",
    "# Call plot_time_series for the 100% stacked plot and add to the second subplot (row 2)\n",
    "fig_100_stacked = plot_time_series(\n",
    "    company_name=uv[\"company_name\"][7],\n",
    "    accounts=[\"01.01.01\", \"01.01.02\", \"01.01.03\", \"01.01.04\"],\n",
    "    plot_type=\"100_stacked\",\n",
    "    start_date=\"2013-12-31\",\n",
    ")\n",
    "\n",
    "# Add the traces from the 100% stacked plot to the second subplot\n",
    "for trace in fig_100_stacked[\"data\"]:\n",
    "    fig.add_trace(trace, row=2, col=1)\n",
    "\n",
    "# Update the layout for the entire figure\n",
    "fig.update_layout(\n",
    "    title_text=\"Comparison of Stacked and 100% Stacked Plots\",\n",
    "    height=800,\n",
    "    showlegend=False,  # Hide the legend since it might overlap\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def plot_company_comparison(\n",
    "    df, companies, account, start_date=None, end_date=None, plot_type=\"lines\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare a financial metric (account) across multiple companies, filtered by 'df_type',\n",
    "    and display the average of the companies.\n",
    "\n",
    "    Args:\n",
    "    - df: DataFrame containing the financial data.\n",
    "    - companies: List of company names to compare.\n",
    "    - account: The financial account to be plotted (e.g., '03.11 - Lucro do Per√≠odo').\n",
    "    - start_date: Run date for filtering the data (optional).\n",
    "    - end_date: End date for filtering the data (optional).\n",
    "    - plot_type: Determines whether to plot 'lines' or 'normalized'.\n",
    "    \"\"\"\n",
    "    # Try 'DFs Consolidadas' first\n",
    "    if (df[\"type\"] == \"DFs Consolidadas\").any():\n",
    "        df_type = \"DFs Consolidadas\"\n",
    "    else:\n",
    "        df_type = \"DFs Individuais\"\n",
    "\n",
    "    # Filter data for the specified companies, account, and df_type\n",
    "    data = df[\n",
    "        (df[\"company_name\"].isin(companies))\n",
    "        & (df[\"account\"] == account)\n",
    "        & (df[\"type\"] == df_type)\n",
    "    ].copy()  # Make a deep copy to avoid SettingWithCopyWarning\n",
    "\n",
    "    # Apply start and end date filters if provided\n",
    "    if start_date:\n",
    "        data = data[\n",
    "            data[\"quarter\"] >= pd.to_datetime(start_date)\n",
    "        ].copy()  # Copy after filtering\n",
    "    if end_date:\n",
    "        data = data[\n",
    "            data[\"quarter\"] <= pd.to_datetime(end_date)\n",
    "        ].copy()  # Copy after filtering\n",
    "\n",
    "    if data.empty:\n",
    "        print(\"No data available for the given filters.\")\n",
    "        return\n",
    "\n",
    "    # Get the description dynamically from the DataFrame based on the 'account' field\n",
    "    description = (\n",
    "        data[\"description\"].dropna().iloc[0]\n",
    "        if \"description\" in data.columns and not data[\"description\"].isna().all()\n",
    "        else account\n",
    "    )\n",
    "\n",
    "    # Create the figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add line plot for each company (no markers)\n",
    "    for company in companies:\n",
    "        company_data = (\n",
    "            data[data[\"company_name\"] == company].sort_values(\"quarter\").copy()\n",
    "        )  # Avoid modifying the original DataFrame\n",
    "\n",
    "        if not company_data.empty:\n",
    "            if plot_type == \"normalized\":\n",
    "                # Try to get the first non-zero value for normalization\n",
    "                non_zero_values = company_data[\"value\"][company_data[\"value\"] != 0]\n",
    "\n",
    "                if not non_zero_values.empty:\n",
    "                    first_non_zero_value = non_zero_values.iloc[0]\n",
    "                    # Normalize the values: scale from 1 to proportional max_value\n",
    "                    company_data.loc[:, \"normalized_value\"] = (\n",
    "                        company_data[\"value\"] / first_non_zero_value\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"Warning: No non-zero values for {company}. Skipping normalization.\"\n",
    "                    )\n",
    "                    company_data.loc[:, \"normalized_value\"] = company_data[\n",
    "                        \"value\"\n",
    "                    ]  # No normalization applied\n",
    "\n",
    "                # Plot normalized lines\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=company_data[\"quarter\"],\n",
    "                        y=company_data[\"normalized_value\"],\n",
    "                        mode=\"lines\",\n",
    "                        name=f\"{company} (normalized)\",\n",
    "                        hoverinfo=\"x+y\",\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                # Plot regular lines\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=company_data[\"quarter\"],\n",
    "                        y=company_data[\"value\"],\n",
    "                        mode=\"lines\",\n",
    "                        name=company,\n",
    "                        hoverinfo=\"x+y\",\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    if plot_type == \"normalized\":\n",
    "        # Calculate the average of actual values for each quarter\n",
    "        average_data = data.groupby(\"quarter\")[\"value\"].mean().reset_index()\n",
    "\n",
    "        # Normalize the average value (just like individual company values)\n",
    "        first_non_zero_avg = average_data[\"value\"][average_data[\"value\"] != 0].iloc[0]\n",
    "        average_data[\"normalized_value\"] = average_data[\"value\"] / first_non_zero_avg\n",
    "\n",
    "        # Add the average normalized line to the plot\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=average_data[\"quarter\"],\n",
    "                y=average_data[\"normalized_value\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"Average (normalized)\",\n",
    "                line=dict(dash=\"dash\", color=\"black\"),\n",
    "                hoverinfo=\"x+y\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        y_axis_title = \"Normalized Value (Proportional to 1)\"\n",
    "    else:\n",
    "        # Calculate the average value for each quarter\n",
    "        average_data = data.groupby(\"quarter\")[\"value\"].mean().reset_index()\n",
    "\n",
    "        # Add the average line to the plot\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=average_data[\"quarter\"],\n",
    "                y=average_data[\"value\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"Average\",\n",
    "                line=dict(dash=\"dash\", color=\"black\"),\n",
    "                hoverinfo=\"x+y\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        y_axis_title = description\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Comparison of {account} - {description}\",\n",
    "        xaxis_title=\"Quarter\",\n",
    "        yaxis_title=y_axis_title,\n",
    "        template=\"plotly_white\",\n",
    "        showlegend=True,\n",
    "    )\n",
    "\n",
    "    # Show plot\n",
    "    fig.show()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "data = plot_company_comparison(\n",
    "    df=df,  # Your DataFrame containing the financial data\n",
    "    companies=[\"OI SA\", \"TELEFONICA BRASIL SA\", \"TIM SA\"],\n",
    "    account=\"01.01.01\",  # Example account code for \"Lucro do Per√≠odo\"\n",
    "    start_date=\"2013-12-31\",  # Filter from this date\n",
    "    end_date=\"2020-12-31\",  # Optional: Filter until this date\n",
    "    plot_type=\"lines\",  # Choose between \"lines\" and \"normalized\"\n",
    ")\n",
    "# Example usage:\n",
    "data = plot_company_comparison(\n",
    "    df=df,  # Your DataFrame containing the financial data\n",
    "    companies=[\"OI SA\", \"TELEFONICA BRASIL SA\", \"TIM SA\"],\n",
    "    account=\"03.01\",  # Example account code for \"Lucro do Per√≠odo\"\n",
    "    # start_date='2020-12-31',  # Filter from this date\n",
    "    # end_date='2023-12-31',     # Optional: Filter until this date\n",
    "    plot_type=\"normalized\",  # Choose between \"lines\" and \"normalized\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie = \"1\"\n",
    "\n",
    "url = f\"https://api.bcb.gov.br/dados/serie/bcdata.sgs.{serie}/dados?formato=json\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_median_data(df):\n",
    "    # Remover a informa√ß√£o de fuso hor√°rio (timezone) antes de converter para per√≠odos\n",
    "    df.index = df.index.tz_localize(None)\n",
    "\n",
    "    # Criar uma coluna de trimestre ('Quarter')\n",
    "    df[\"quarter\"] = df.index.to_period(\"Q\")\n",
    "\n",
    "    # Calcular a mediana de 'Adj Close' por trimestre\n",
    "    median = df.groupby(\"quarter\")[\"value\"].median()\n",
    "    # mean = df.groupby('quarter')['Adj Close'].mean()\n",
    "\n",
    "    # Convert last_dates to the actual last date of the quarter\n",
    "    # Using the 'Q' period information from 'Quarter'\n",
    "    last_quarter_dates = df[\"quarter\"].map(lambda x: x.end_time)\n",
    "\n",
    "    # Converter as datas para manter apenas a parte da data (sem tempo)\n",
    "    last_quarter_dates = last_quarter_dates.dt.date.drop_duplicates()\n",
    "\n",
    "    # Criar um DataFrame com a √∫ltima data correta de cada trimestre e a mediana trimestral\n",
    "    quarterly_summary = pd.DataFrame(\n",
    "        {\n",
    "            \"quarter\": last_quarter_dates.values,  # Drop duplicates to keep one last date per quarter\n",
    "            \"median\": median.values,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return quarterly_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm = [\n",
    "    \"ELETROMIDIA SA\",\n",
    "    \"ALGAR TELECOM SA\",\n",
    "    \"BRISANET PARTICIPACOES SA\",\n",
    "    \"DESKTOP SA\",\n",
    "    \"GIGA MAIS FIBRA TELECOMUNICACOES SA\",\n",
    "    \"OI SA\",\n",
    "    \"TELEC BRASILEIRAS SA TELEBRAS\",\n",
    "    \"TELEFONICA BRASIL SA\",\n",
    "    \"TIM SA\",\n",
    "    \"UNIFIQUE TELECOMUNICACOES SA\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sector = 'COMUNICACOES'\n",
    "db_file = f\"..\\data\\{settings.db_filepath}\"\n",
    "\n",
    "conn = sqlite3.connect(db_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Fetch all table names excluding internal SQLite tables\n",
    "cursor.execute(\n",
    "    \"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';\"\n",
    ")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "df = pd.read_sql_query(f\"SELECT * FROM company_info\", conn)\n",
    "companies = df[df[\"ticker_codes\"] != \"\"]\n",
    "companies = companies[\n",
    "    [\"cvm_code\", \"company_name\", \"ticker\", \"ticker_codes\"]\n",
    "].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique ticker codes from the 'ticker_codes' column\n",
    "ticker_codes = companies[\"ticker_codes\"].unique()\n",
    "\n",
    "# Split the comma-separated ticker codes, flatten the list, and append '.SA' if necessary\n",
    "ticker_list = [\n",
    "    f\"{code.strip()}.SA\" if not code.strip().endswith(\".SA\") else code.strip()\n",
    "    for tickers in ticker_codes\n",
    "    for code in tickers.split(\",\")\n",
    "    if code\n",
    "]\n",
    "\n",
    "ticker_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique ticker codes from the 'ticker_codes' column\n",
    "ticker_codes = companies[\"ticker_codes\"].unique()\n",
    "\n",
    "# Split the comma-separated ticker codes, flatten the list, and append '.SA' if necessary\n",
    "ticker_list = [\n",
    "    f\"{code.strip()}.SA\" if not code.strip().endswith(\".SA\") else code.strip()\n",
    "    for tickers in ticker_codes\n",
    "    for code in tickers.split(\",\")\n",
    "    if code\n",
    "]\n",
    "\n",
    "# Remove empty strings (if any) and display the result\n",
    "companies = [code for code in ticker_list if code]\n",
    "len(companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixar os dados\n",
    "last_date = \"1900-01-01\"\n",
    "# companies = ['AAPL', 'ITUB3.SA', 'ITUB4.SA', 'GOOG']\n",
    "data = yf.download(\" \".join(companies), start=last_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_data = {}\n",
    "companies2 = []\n",
    "data_error = {}\n",
    "start_time = time.time()\n",
    "for i, company in enumerate(companies):\n",
    "    try:\n",
    "        # Selecionar os dados de company e remover NaNs\n",
    "        df = data.xs(company, axis=1, level=1).dropna()\n",
    "        df[\"value\"] = df[\"Adj Close\"]\n",
    "\n",
    "        # get median data from df\n",
    "        df = get_median_data(df)\n",
    "\n",
    "        historical_data[company] = df\n",
    "        companies2.append(company)\n",
    "        extra_info = [company]\n",
    "        print_info(i, extra_info, start_time, len(companies))\n",
    "    except Exception as e:\n",
    "        data_error[company] = company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now find the overall min and max quarter across all tickers\n",
    "all_quarters = [data[\"quarter\"] for data in historical_data.values()]\n",
    "max_quarter = pd.concat(all_quarters).max()\n",
    "previous_quarter = pd.Period(max_quarter, freq=\"Q\") - 1\n",
    "last_date = previous_quarter.end_time.date()\n",
    "\n",
    "data2 = yf.download(\" \".join(companies2), start=last_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yahoo Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(files):\n",
    "    \"\"\"\n",
    "    Run financial data from the database and process it into DataFrames.\n",
    "\n",
    "    Args:\n",
    "        files (str): The name part of the database file to load.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are sectors and values are DataFrames containing the NSD data for that sector.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        db_file = os.path.join(\n",
    "            settings.data_folder, f\"{settings.db_filepath.split('.')[0]} {files}.db\"\n",
    "        )\n",
    "        db_file = \"D:/Fausto Stangler/Documentos/Python/FLY/backend/data/b3 standard.db\"\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Fetch all table names excluding internal SQLite tables\n",
    "        cursor.execute(\n",
    "            \"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';\"\n",
    "        )\n",
    "        tables = cursor.fetchall()\n",
    "\n",
    "        dfs = {}\n",
    "        total_lines = 0\n",
    "        start_time = time.time()  # Initialize start time for progress tracking\n",
    "\n",
    "        # Iterate through each table (sector) and process the data\n",
    "        for i, table in enumerate(tables):\n",
    "            try:\n",
    "                sector = table[0]\n",
    "                df = pd.read_sql_query(f\"SELECT * FROM {sector}\", conn)\n",
    "\n",
    "                # Normalize date columns to datetime format\n",
    "                df[\"quarter\"] = pd.to_datetime(df[\"quarter\"], errors=\"coerce\")\n",
    "\n",
    "                # Normalize numeric columns\n",
    "                df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
    "\n",
    "                # Fill missing 'value' with 0\n",
    "                df[\"value\"] = df[\"value\"].fillna(0)\n",
    "\n",
    "                # Identify rows where 'account' is missing or NaN and 'value' has been set to 0\n",
    "                missing_account = df[\"account\"].isna() | df[\"account\"].str.strip().eq(\n",
    "                    \"\"\n",
    "                )\n",
    "                df.loc[missing_account, \"account\"] = (\n",
    "                    \"0\"  # Set 'account' to '0' (as text) for these rows\n",
    "                )\n",
    "\n",
    "                # Filter out only the latest versions for each group\n",
    "                # df, _ = self.filter_newer_versions(df)\n",
    "                dfs[sector] = df  # Store the DataFrame with the sector as the key\n",
    "                total_lines += len(df)  # Update the total number of processed lines\n",
    "\n",
    "                # Display progress\n",
    "                extra_info = [\n",
    "                    f\"Loaded {len(df)} items from {sector} in {files}, total {total_lines}\"\n",
    "                ]\n",
    "                print_info(\n",
    "                    i, extra_info, start_time, len(tables)\n",
    "                )  # Removed the total_files argument\n",
    "\n",
    "                print(\"break\")\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                # system.log_error(f\"Error processing table {table}: {e}\")\n",
    "                print(e)\n",
    "\n",
    "        conn.close()\n",
    "        return dfs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # system.log_error(f\"Error loading existing financial statements: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_company_data():\n",
    "\n",
    "    company_data = {}\n",
    "\n",
    "    try:\n",
    "        db_file = os.path.join(settings.data_folder, f\"{settings.db_filepath}\")\n",
    "        db_file = \"D:/Fausto Stangler/Documentos/Python/FLY/backend/data/b3.db\"\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        cursor.execute(f\"SELECT * FROM {settings.company_table}\")\n",
    "\n",
    "        for row in cursor.fetchall():\n",
    "            company_name = row[settings.company_columns.index(\"company_name\")]\n",
    "            company_data[company_name] = dict(zip(settings.company_columns, row))\n",
    "\n",
    "    except Exception as e:\n",
    "        # system.log_error(e)\n",
    "        print(e)\n",
    "\n",
    "    return company_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_median_data(df):\n",
    "    # Remover a informa√ß√£o de fuso hor√°rio (timezone) antes de converter para per√≠odos\n",
    "    df.index = df.index.tz_localize(None)\n",
    "\n",
    "    # Criar uma coluna de trimestre ('Quarter')\n",
    "    df[\"quarter\"] = df.index.to_period(\"Q\")\n",
    "\n",
    "    # Calcular a mediana de 'Adj Close' por trimestre\n",
    "    median = df.groupby(\"quarter\")[\"value\"].median()\n",
    "    # mean = df.groupby('quarter')['Adj Close'].mean()\n",
    "\n",
    "    # Convert last_dates to the actual last date of the quarter\n",
    "    # Using the 'Q' period information from 'Quarter'\n",
    "    last_quarter_dates = df[\"quarter\"].map(lambda x: x.end_time)\n",
    "\n",
    "    # Converter as datas para manter apenas a parte da data (sem tempo)\n",
    "    last_quarter_dates = last_quarter_dates.dt.date.drop_duplicates()\n",
    "\n",
    "    # Criar um DataFrame com a √∫ltima data correta de cada trimestre e a mediana trimestral\n",
    "    quarterly_summary = pd.DataFrame(\n",
    "        {\n",
    "            \"quarter\": last_quarter_dates.values,  # Drop duplicates to keep one last date per quarter\n",
    "            \"median\": median.values,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return quarterly_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.14% 1+13, 0.480669s per item, Time: 0h 00m 00s + 0h 00m 06s Loaded 90669 items from COMUNICACOES in standard, total 90669\n",
      "break\n"
     ]
    }
   ],
   "source": [
    "statements_data = load_data(settings.statements_standard)\n",
    "df_statements = statements_data[\"COMUNICACOES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_companies = load_company_data()\n",
    "df_companies = pd.DataFrame(all_companies).T.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge statements and company info\n",
    "\n",
    "columns = [\"company_name\"]\n",
    "cols_to_merge = columns + [\n",
    "    \"cvm_code\",\n",
    "    \"ticker\",\n",
    "    \"ticker_codes\",\n",
    "    \"isin_codes\",\n",
    "    \"listing\",\n",
    "]\n",
    "# Merge the two DataFrames on the 'company_name' column\n",
    "df_statements_companies = pd.merge(\n",
    "    df_statements, df_companies[cols_to_merge], on=columns, how=\"left\"\n",
    ")\n",
    "\n",
    "list_of_tickers = df_statements_companies[\n",
    "    [\"company_name\", \"ticker_codes\"]\n",
    "].drop_duplicates()\n",
    "list_of_quarters = df_statements_companies[\n",
    "    [\"company_name\", \"ticker_codes\", \"quarter\"]\n",
    "].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELETROMIDIA SA ELMD3\n",
      "BRISANET PARTICIPACOES SA BRIT3\n",
      "DESKTOP SA DESK3\n",
      "OI SA OIBR3\n",
      "OI SA OIBR4\n",
      "TELEC BRASILEIRAS SA TELEBRAS TELB3\n",
      "TELEC BRASILEIRAS SA TELEBRAS TELB4\n",
      "TELEFONICA BRASIL SA VIVT3\n",
      "TIM SA TIMS3\n",
      "UNIFIQUE TELECOMUNICACOES SA FIQE3\n"
     ]
    }
   ],
   "source": [
    "# get historical data from yahoo finance\n",
    "\n",
    "historical_data = {}\n",
    "last_date = \"1950-01-01\"\n",
    "\n",
    "# Iterate over each row and process tickers\n",
    "for index, row in list_of_tickers.iterrows():\n",
    "    company_name = row[\"company_name\"]\n",
    "    tickers = row[\"ticker_codes\"]\n",
    "\n",
    "    # Ensure that tickers are passed as a list by splitting any comma-separated string\n",
    "    tickers = tickers.split(\",\") if isinstance(tickers, str) else tickers\n",
    "\n",
    "    for ticker in tickers:\n",
    "\n",
    "        if ticker:\n",
    "            print(company_name, ticker)\n",
    "            df = yf.download(\n",
    "                ticker + \".SA\", start=last_date, group_by=\"ticker\", progress=False\n",
    "            )\n",
    "            df[\"value\"] = df[\"Adj Close\"]\n",
    "            df = get_median_data(df)\n",
    "\n",
    "        historical_data[ticker] = df\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELETROMIDIA SA 2019-12-31 00:00:00 ELMD 3\n",
      "ELETROMIDIA SA 2020-03-31 00:00:00 ELMD 3\n",
      "ELETROMIDIA SA 2020-06-30 00:00:00 ELMD 3\n",
      "ELETROMIDIA SA 2020-09-30 00:00:00 ELMD 3\n",
      "ELETROMIDIA SA 2020-12-31 00:00:00 ELMD 3\n",
      "ELETROMIDIA SA 2021-03-31 00:00:00 ELMD 3\n",
      "ELETROMIDIA SA 2021-06-30 00:00:00 ELMD 3\n",
      "ELETROMIDIA SA 2021-09-30 00:00:00 ELMD 3\n",
      "ELETROMIDIA SA 2021-12-31 00:00:00 ELMD 3\n",
      "ELETROMIDIA SA 2022-03-31 00:00:00 ELMD 3\n",
      "ELETROMIDIA SA 2022-06-30 00:00:00 ELMD 3\n",
      "ELETROMIDIA SA 2022-09-30 00:00:00 ELMD 3\n",
      "ELETROMIDIA SA 2022-12-31 00:00:00 ELMD 3\n",
      "ELETROMIDIA SA 2023-03-31 00:00:00 ELMD 3\n",
      "ELETROMIDIA SA 2023-06-30 00:00:00 ELMD 3\n",
      "ELETROMIDIA SA 2023-09-30 00:00:00 ELMD 3\n",
      "ELETROMIDIA SA 2023-12-31 00:00:00 ELMD 3\n",
      "ELETROMIDIA SA 2024-03-31 00:00:00 ELMD 3\n",
      "ELETROMIDIA SA 2024-06-30 00:00:00 ELMD 3\n",
      "BRISANET PARTICIPACOES SA 2020-12-31 00:00:00 BRIT 3\n",
      "BRISANET PARTICIPACOES SA 2021-03-31 00:00:00 BRIT 3\n",
      "BRISANET PARTICIPACOES SA 2021-06-30 00:00:00 BRIT 3\n",
      "BRISANET PARTICIPACOES SA 2021-09-30 00:00:00 BRIT 3\n",
      "BRISANET PARTICIPACOES SA 2021-12-31 00:00:00 BRIT 3\n",
      "BRISANET PARTICIPACOES SA 2022-03-31 00:00:00 BRIT 3\n",
      "BRISANET PARTICIPACOES SA 2022-06-30 00:00:00 BRIT 3\n",
      "BRISANET PARTICIPACOES SA 2022-09-30 00:00:00 BRIT 3\n",
      "BRISANET PARTICIPACOES SA 2022-12-31 00:00:00 BRIT 3\n",
      "BRISANET PARTICIPACOES SA 2023-03-31 00:00:00 BRIT 3\n",
      "BRISANET PARTICIPACOES SA 2023-06-30 00:00:00 BRIT 3\n",
      "BRISANET PARTICIPACOES SA 2023-09-30 00:00:00 BRIT 3\n",
      "BRISANET PARTICIPACOES SA 2023-12-31 00:00:00 BRIT 3\n",
      "BRISANET PARTICIPACOES SA 2024-03-31 00:00:00 BRIT 3\n",
      "BRISANET PARTICIPACOES SA 2024-06-30 00:00:00 BRIT 3\n",
      "DESKTOP SA 2020-12-31 00:00:00 DESK 3\n",
      "DESKTOP SA 2021-03-31 00:00:00 DESK 3\n",
      "DESKTOP SA 2021-06-30 00:00:00 DESK 3\n",
      "DESKTOP SA 2021-09-30 00:00:00 DESK 3\n",
      "DESKTOP SA 2021-12-31 00:00:00 DESK 3\n",
      "DESKTOP SA 2022-03-31 00:00:00 DESK 3\n",
      "DESKTOP SA 2022-06-30 00:00:00 DESK 3\n",
      "DESKTOP SA 2022-09-30 00:00:00 DESK 3\n",
      "DESKTOP SA 2022-12-31 00:00:00 DESK 3\n",
      "DESKTOP SA 2023-03-31 00:00:00 DESK 3\n",
      "DESKTOP SA 2023-06-30 00:00:00 DESK 3\n",
      "DESKTOP SA 2023-09-30 00:00:00 DESK 3\n",
      "DESKTOP SA 2023-12-31 00:00:00 DESK 3\n",
      "DESKTOP SA 2024-03-31 00:00:00 DESK 3\n",
      "DESKTOP SA 2024-06-30 00:00:00 DESK 3\n",
      "OI SA 2010-12-31 00:00:00 OIBR 3\n",
      "OI SA 2010-12-31 00:00:00 OIBR 4\n",
      "OI SA 2011-03-31 00:00:00 OIBR 3\n",
      "OI SA 2011-03-31 00:00:00 OIBR 4\n",
      "OI SA 2011-06-30 00:00:00 OIBR 3\n",
      "OI SA 2011-06-30 00:00:00 OIBR 4\n",
      "OI SA 2011-09-30 00:00:00 OIBR 3\n",
      "OI SA 2011-09-30 00:00:00 OIBR 4\n",
      "OI SA 2011-12-31 00:00:00 OIBR 3\n",
      "OI SA 2011-12-31 00:00:00 OIBR 4\n",
      "OI SA 2012-03-31 00:00:00 OIBR 3\n",
      "OI SA 2012-03-31 00:00:00 OIBR 4\n",
      "OI SA 2012-06-30 00:00:00 OIBR 3\n",
      "OI SA 2012-06-30 00:00:00 OIBR 4\n",
      "OI SA 2012-09-30 00:00:00 OIBR 3\n",
      "OI SA 2012-09-30 00:00:00 OIBR 4\n",
      "OI SA 2012-12-31 00:00:00 OIBR 3\n",
      "OI SA 2012-12-31 00:00:00 OIBR 4\n",
      "OI SA 2013-03-31 00:00:00 OIBR 3\n",
      "OI SA 2013-03-31 00:00:00 OIBR 4\n",
      "OI SA 2013-06-30 00:00:00 OIBR 3\n",
      "OI SA 2013-06-30 00:00:00 OIBR 4\n",
      "OI SA 2013-09-30 00:00:00 OIBR 3\n",
      "OI SA 2013-09-30 00:00:00 OIBR 4\n",
      "OI SA 2013-12-31 00:00:00 OIBR 3\n",
      "OI SA 2013-12-31 00:00:00 OIBR 4\n",
      "OI SA 2014-03-31 00:00:00 OIBR 3\n",
      "OI SA 2014-03-31 00:00:00 OIBR 4\n",
      "OI SA 2014-06-30 00:00:00 OIBR 3\n",
      "OI SA 2014-06-30 00:00:00 OIBR 4\n",
      "OI SA 2014-09-30 00:00:00 OIBR 3\n",
      "OI SA 2014-09-30 00:00:00 OIBR 4\n",
      "OI SA 2014-12-31 00:00:00 OIBR 3\n",
      "OI SA 2014-12-31 00:00:00 OIBR 4\n",
      "OI SA 2015-03-31 00:00:00 OIBR 3\n",
      "OI SA 2015-03-31 00:00:00 OIBR 4\n",
      "OI SA 2015-06-30 00:00:00 OIBR 3\n",
      "OI SA 2015-06-30 00:00:00 OIBR 4\n",
      "OI SA 2015-09-30 00:00:00 OIBR 3\n",
      "OI SA 2015-09-30 00:00:00 OIBR 4\n",
      "OI SA 2015-12-31 00:00:00 OIBR 3\n",
      "OI SA 2015-12-31 00:00:00 OIBR 4\n",
      "OI SA 2016-03-31 00:00:00 OIBR 3\n",
      "OI SA 2016-03-31 00:00:00 OIBR 4\n",
      "OI SA 2016-06-30 00:00:00 OIBR 3\n",
      "OI SA 2016-06-30 00:00:00 OIBR 4\n",
      "OI SA 2016-09-30 00:00:00 OIBR 3\n",
      "OI SA 2016-09-30 00:00:00 OIBR 4\n",
      "OI SA 2016-12-31 00:00:00 OIBR 3\n",
      "OI SA 2016-12-31 00:00:00 OIBR 4\n",
      "OI SA 2017-03-31 00:00:00 OIBR 3\n",
      "OI SA 2017-03-31 00:00:00 OIBR 4\n",
      "OI SA 2017-06-30 00:00:00 OIBR 3\n",
      "OI SA 2017-06-30 00:00:00 OIBR 4\n",
      "OI SA 2017-09-30 00:00:00 OIBR 3\n",
      "OI SA 2017-09-30 00:00:00 OIBR 4\n",
      "OI SA 2017-12-31 00:00:00 OIBR 3\n",
      "OI SA 2017-12-31 00:00:00 OIBR 4\n",
      "OI SA 2018-03-31 00:00:00 OIBR 3\n",
      "OI SA 2018-03-31 00:00:00 OIBR 4\n",
      "OI SA 2018-06-30 00:00:00 OIBR 3\n",
      "OI SA 2018-06-30 00:00:00 OIBR 4\n",
      "OI SA 2018-09-30 00:00:00 OIBR 3\n",
      "OI SA 2018-09-30 00:00:00 OIBR 4\n",
      "OI SA 2018-12-31 00:00:00 OIBR 3\n",
      "OI SA 2018-12-31 00:00:00 OIBR 4\n",
      "OI SA 2019-03-31 00:00:00 OIBR 3\n",
      "OI SA 2019-03-31 00:00:00 OIBR 4\n",
      "OI SA 2019-06-30 00:00:00 OIBR 3\n",
      "OI SA 2019-06-30 00:00:00 OIBR 4\n",
      "OI SA 2019-09-30 00:00:00 OIBR 3\n",
      "OI SA 2019-09-30 00:00:00 OIBR 4\n",
      "OI SA 2019-12-31 00:00:00 OIBR 3\n",
      "OI SA 2019-12-31 00:00:00 OIBR 4\n",
      "OI SA 2020-03-31 00:00:00 OIBR 3\n",
      "OI SA 2020-03-31 00:00:00 OIBR 4\n",
      "OI SA 2020-06-30 00:00:00 OIBR 3\n",
      "OI SA 2020-06-30 00:00:00 OIBR 4\n",
      "OI SA 2020-09-30 00:00:00 OIBR 3\n",
      "OI SA 2020-09-30 00:00:00 OIBR 4\n",
      "OI SA 2020-12-31 00:00:00 OIBR 3\n",
      "OI SA 2020-12-31 00:00:00 OIBR 4\n",
      "OI SA 2021-03-31 00:00:00 OIBR 3\n",
      "OI SA 2021-03-31 00:00:00 OIBR 4\n",
      "OI SA 2021-06-30 00:00:00 OIBR 3\n",
      "OI SA 2021-06-30 00:00:00 OIBR 4\n",
      "OI SA 2021-09-30 00:00:00 OIBR 3\n",
      "OI SA 2021-09-30 00:00:00 OIBR 4\n",
      "OI SA 2021-12-31 00:00:00 OIBR 3\n",
      "OI SA 2021-12-31 00:00:00 OIBR 4\n",
      "OI SA 2022-03-31 00:00:00 OIBR 3\n",
      "OI SA 2022-03-31 00:00:00 OIBR 4\n",
      "OI SA 2022-06-30 00:00:00 OIBR 3\n",
      "OI SA 2022-06-30 00:00:00 OIBR 4\n",
      "OI SA 2022-09-30 00:00:00 OIBR 3\n",
      "OI SA 2022-09-30 00:00:00 OIBR 4\n",
      "OI SA 2022-12-31 00:00:00 OIBR 3\n",
      "OI SA 2022-12-31 00:00:00 OIBR 4\n",
      "OI SA 2023-03-31 00:00:00 OIBR 3\n",
      "OI SA 2023-03-31 00:00:00 OIBR 4\n",
      "OI SA 2023-06-30 00:00:00 OIBR 3\n",
      "OI SA 2023-06-30 00:00:00 OIBR 4\n",
      "OI SA 2023-09-30 00:00:00 OIBR 3\n",
      "OI SA 2023-09-30 00:00:00 OIBR 4\n",
      "OI SA 2023-12-31 00:00:00 OIBR 3\n",
      "OI SA 2023-12-31 00:00:00 OIBR 4\n",
      "OI SA 2024-03-31 00:00:00 OIBR 3\n",
      "OI SA 2024-03-31 00:00:00 OIBR 4\n",
      "OI SA 2024-06-30 00:00:00 OIBR 3\n",
      "OI SA 2024-06-30 00:00:00 OIBR 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2010-12-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2010-12-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2011-03-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2011-03-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2011-06-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2011-06-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2011-09-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2011-09-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2011-12-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2011-12-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2012-03-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2012-03-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2012-06-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2012-06-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2012-09-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2012-09-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2012-12-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2012-12-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2013-03-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2013-03-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2013-06-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2013-06-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2013-09-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2013-09-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2013-12-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2013-12-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2014-03-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2014-03-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2014-06-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2014-06-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2014-09-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2014-09-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2014-12-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2014-12-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2015-03-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2015-03-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2015-06-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2015-06-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2015-09-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2015-09-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2015-12-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2015-12-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2016-03-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2016-03-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2016-06-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2016-06-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2016-09-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2016-09-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2016-12-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2016-12-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2017-03-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2017-03-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2017-06-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2017-06-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2017-09-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2017-09-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2017-12-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2017-12-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2018-03-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2018-03-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2018-06-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2018-06-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2018-09-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2018-09-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2018-12-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2018-12-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2019-03-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2019-03-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2019-06-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2019-06-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2019-09-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2019-09-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2019-12-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2019-12-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2020-03-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2020-03-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2020-06-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2020-06-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2020-09-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2020-09-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2020-12-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2020-12-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2021-03-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2021-03-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2021-06-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2021-06-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2021-09-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2021-09-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2021-12-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2021-12-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2022-03-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2022-03-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2022-06-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2022-06-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2022-09-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2022-09-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2022-12-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2022-12-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2023-03-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2023-03-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2023-06-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2023-06-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2023-09-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2023-09-30 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2023-12-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2023-12-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2024-03-31 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2024-03-31 00:00:00 TELB 4\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2024-06-30 00:00:00 TELB 3\n",
      "TELEC BRASILEIRAS SA TELEBRAS 2024-06-30 00:00:00 TELB 4\n",
      "TELEFONICA BRASIL SA 2010-12-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2011-03-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2011-06-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2011-09-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2011-12-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2012-03-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2012-06-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2012-09-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2012-12-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2013-03-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2013-06-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2013-09-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2013-12-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2014-03-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2014-06-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2014-09-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2014-12-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2015-03-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2015-06-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2015-09-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2015-12-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2016-03-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2016-06-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2016-09-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2016-12-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2017-03-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2017-06-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2017-09-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2017-12-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2018-03-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2018-06-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2018-09-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2018-12-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2019-03-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2019-06-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2019-09-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2019-12-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2020-03-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2020-06-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2020-09-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2020-12-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2021-03-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2021-06-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2021-09-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2021-12-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2022-03-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2022-06-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2022-09-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2022-12-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2023-03-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2023-06-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2023-09-30 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2023-12-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2024-03-31 00:00:00 VIVT 3\n",
      "TELEFONICA BRASIL SA 2024-06-30 00:00:00 VIVT 3\n",
      "TIM SA 2018-12-31 00:00:00 TIMS 3\n",
      "TIM SA 2019-03-31 00:00:00 TIMS 3\n",
      "TIM SA 2019-06-30 00:00:00 TIMS 3\n",
      "TIM SA 2019-09-30 00:00:00 TIMS 3\n",
      "TIM SA 2019-12-31 00:00:00 TIMS 3\n",
      "TIM SA 2020-03-31 00:00:00 TIMS 3\n",
      "TIM SA 2020-06-30 00:00:00 TIMS 3\n",
      "TIM SA 2020-09-30 00:00:00 TIMS 3\n",
      "TIM SA 2020-12-31 00:00:00 TIMS 3\n",
      "TIM SA 2021-03-31 00:00:00 TIMS 3\n",
      "TIM SA 2021-06-30 00:00:00 TIMS 3\n",
      "TIM SA 2021-09-30 00:00:00 TIMS 3\n",
      "TIM SA 2021-12-31 00:00:00 TIMS 3\n",
      "TIM SA 2022-03-31 00:00:00 TIMS 3\n",
      "TIM SA 2022-06-30 00:00:00 TIMS 3\n",
      "TIM SA 2022-09-30 00:00:00 TIMS 3\n",
      "TIM SA 2022-12-31 00:00:00 TIMS 3\n",
      "TIM SA 2023-03-31 00:00:00 TIMS 3\n",
      "TIM SA 2023-06-30 00:00:00 TIMS 3\n",
      "TIM SA 2023-09-30 00:00:00 TIMS 3\n",
      "TIM SA 2023-12-31 00:00:00 TIMS 3\n",
      "TIM SA 2024-03-31 00:00:00 TIMS 3\n",
      "TIM SA 2024-06-30 00:00:00 TIMS 3\n",
      "UNIFIQUE TELECOMUNICACOES SA 2020-12-31 00:00:00 FIQE 3\n",
      "UNIFIQUE TELECOMUNICACOES SA 2021-03-31 00:00:00 FIQE 3\n",
      "UNIFIQUE TELECOMUNICACOES SA 2021-06-30 00:00:00 FIQE 3\n",
      "UNIFIQUE TELECOMUNICACOES SA 2021-09-30 00:00:00 FIQE 3\n",
      "UNIFIQUE TELECOMUNICACOES SA 2021-12-31 00:00:00 FIQE 3\n",
      "UNIFIQUE TELECOMUNICACOES SA 2022-03-31 00:00:00 FIQE 3\n",
      "UNIFIQUE TELECOMUNICACOES SA 2022-06-30 00:00:00 FIQE 3\n",
      "UNIFIQUE TELECOMUNICACOES SA 2022-09-30 00:00:00 FIQE 3\n",
      "UNIFIQUE TELECOMUNICACOES SA 2022-12-31 00:00:00 FIQE 3\n",
      "UNIFIQUE TELECOMUNICACOES SA 2023-03-31 00:00:00 FIQE 3\n",
      "UNIFIQUE TELECOMUNICACOES SA 2023-06-30 00:00:00 FIQE 3\n",
      "UNIFIQUE TELECOMUNICACOES SA 2023-09-30 00:00:00 FIQE 3\n",
      "UNIFIQUE TELECOMUNICACOES SA 2023-12-31 00:00:00 FIQE 3\n",
      "UNIFIQUE TELECOMUNICACOES SA 2024-03-31 00:00:00 FIQE 3\n",
      "UNIFIQUE TELECOMUNICACOES SA 2024-06-30 00:00:00 FIQE 3\n"
     ]
    }
   ],
   "source": [
    "# create new historical data rows\n",
    "\n",
    "new_rows = []\n",
    "for i, row in list_of_quarters.iterrows():\n",
    "    company_name = row[\"company_name\"]\n",
    "    quarter = row[\"quarter\"]\n",
    "    tickers = row[\"ticker_codes\"]\n",
    "\n",
    "    tickers = tickers.split(\",\") if isinstance(tickers, str) else tickers\n",
    "    for ticker in tickers:\n",
    "        if ticker:\n",
    "            # Separate ticker into no digits and only digits\n",
    "            tick = \"\".join(\n",
    "                [char for char in ticker if not char.isdigit()]\n",
    "            )  # Ticker without digits\n",
    "            ticker_digit = \"\".join(\n",
    "                [char for char in ticker if char.isdigit()]\n",
    "            )  # Only digits\n",
    "\n",
    "            mask = df_statements_companies[\"company_name\"] == company_name\n",
    "            mask &= df_statements_companies[\"quarter\"] == quarter\n",
    "            mask &= df_statements_companies[\"ticker\"] == tick\n",
    "\n",
    "            new_row = df_statements_companies[mask].iloc[0].copy()\n",
    "\n",
    "            # Modify the necessary columns in the copied row\n",
    "            new_row[\"type\"] = \"Cota√ß√µes Hist√≥ricas\"\n",
    "            new_row[\"frame\"] = settings.tipos_acoes[ticker_digit]\n",
    "            new_row[\"account\"] = (\n",
    "                f\"99.{ticker_digit}\"  # ticker_digit is assumed to be extracted from the original ticker\n",
    "            )\n",
    "            new_row[\"description\"] = \"Cota√ß√£o Mediana do Trimestre\"\n",
    "\n",
    "            # Ensure 'quarter' in 'historical_data' is converted to datetime\n",
    "            df_historical_data = historical_data.get(\n",
    "                tick + ticker_digit, pd.DataFrame()\n",
    "            )  # Use .get() to avoid KeyError if key doesn't exist\n",
    "            if df_historical_data.empty:\n",
    "                new_value = pd.NA\n",
    "            else:\n",
    "                df_historical_data[\"quarter\"] = pd.to_datetime(\n",
    "                    df_historical_data[\"quarter\"]\n",
    "                )\n",
    "\n",
    "                # Filter the 'historical_data' for the correct 'quarter' and get the 'median' value\n",
    "                mask = df_historical_data[\"quarter\"] == pd.to_datetime(quarter)\n",
    "                new_value = df_historical_data[mask][\"median\"].values\n",
    "\n",
    "                # If no values are found, set to pd.NA\n",
    "                if len(new_value) == 0:\n",
    "                    new_value = pd.NA\n",
    "                else:\n",
    "                    new_value = new_value[0]  # Extract the actual median value\n",
    "\n",
    "            # Set the new value in the new row\n",
    "            new_row[\"value\"] = new_value\n",
    "\n",
    "            new_rows.append(new_row)\n",
    "            print(company_name, quarter, tick, ticker_digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nsd</th>\n",
       "      <th>sector</th>\n",
       "      <th>subsector</th>\n",
       "      <th>segment</th>\n",
       "      <th>company_name</th>\n",
       "      <th>quarter</th>\n",
       "      <th>version</th>\n",
       "      <th>type</th>\n",
       "      <th>frame</th>\n",
       "      <th>account</th>\n",
       "      <th>description</th>\n",
       "      <th>value</th>\n",
       "      <th>cvm_code</th>\n",
       "      <th>ticker</th>\n",
       "      <th>ticker_codes</th>\n",
       "      <th>isin_codes</th>\n",
       "      <th>listing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99116</td>\n",
       "      <td>COMUNICACOES</td>\n",
       "      <td>MIDIA</td>\n",
       "      <td>PUBLICIDADE E PROPAGANDA</td>\n",
       "      <td>ELETROMIDIA SA</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>1</td>\n",
       "      <td>DFs Consolidadas</td>\n",
       "      <td>Balan√ßo Patrimonial Ativo</td>\n",
       "      <td>01</td>\n",
       "      <td>Ativo Total</td>\n",
       "      <td>2905300000.0</td>\n",
       "      <td>25569</td>\n",
       "      <td>ELMD</td>\n",
       "      <td>ELMD3</td>\n",
       "      <td>BRELMDACNOR3</td>\n",
       "      <td>CIA NOVO MERCADO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99116</td>\n",
       "      <td>COMUNICACOES</td>\n",
       "      <td>MIDIA</td>\n",
       "      <td>PUBLICIDADE E PROPAGANDA</td>\n",
       "      <td>ELETROMIDIA SA</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>1</td>\n",
       "      <td>DFs Consolidadas</td>\n",
       "      <td>Balan√ßo Patrimonial Ativo</td>\n",
       "      <td>01.01</td>\n",
       "      <td>Ativo Circulante de Curto Prazo</td>\n",
       "      <td>1061320000.0</td>\n",
       "      <td>25569</td>\n",
       "      <td>ELMD</td>\n",
       "      <td>ELMD3</td>\n",
       "      <td>BRELMDACNOR3</td>\n",
       "      <td>CIA NOVO MERCADO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99116</td>\n",
       "      <td>COMUNICACOES</td>\n",
       "      <td>MIDIA</td>\n",
       "      <td>PUBLICIDADE E PROPAGANDA</td>\n",
       "      <td>ELETROMIDIA SA</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>1</td>\n",
       "      <td>DFs Consolidadas</td>\n",
       "      <td>Balan√ßo Patrimonial Ativo</td>\n",
       "      <td>01.01.01</td>\n",
       "      <td>Caixa e Equivalentes de Caixa de Curto Prazo</td>\n",
       "      <td>380180000.0</td>\n",
       "      <td>25569</td>\n",
       "      <td>ELMD</td>\n",
       "      <td>ELMD3</td>\n",
       "      <td>BRELMDACNOR3</td>\n",
       "      <td>CIA NOVO MERCADO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99116</td>\n",
       "      <td>COMUNICACOES</td>\n",
       "      <td>MIDIA</td>\n",
       "      <td>PUBLICIDADE E PROPAGANDA</td>\n",
       "      <td>ELETROMIDIA SA</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>1</td>\n",
       "      <td>DFs Consolidadas</td>\n",
       "      <td>Balan√ßo Patrimonial Ativo</td>\n",
       "      <td>01.01.01.01</td>\n",
       "      <td>Caixa e Bancos de Curto Prazo</td>\n",
       "      <td>130980000.0</td>\n",
       "      <td>25569</td>\n",
       "      <td>ELMD</td>\n",
       "      <td>ELMD3</td>\n",
       "      <td>BRELMDACNOR3</td>\n",
       "      <td>CIA NOVO MERCADO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99116</td>\n",
       "      <td>COMUNICACOES</td>\n",
       "      <td>MIDIA</td>\n",
       "      <td>PUBLICIDADE E PROPAGANDA</td>\n",
       "      <td>ELETROMIDIA SA</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>1</td>\n",
       "      <td>DFs Consolidadas</td>\n",
       "      <td>Balan√ßo Patrimonial Ativo</td>\n",
       "      <td>01.01.01.02</td>\n",
       "      <td>Aplica√ß√µes L√≠quidas de Curto Prazo</td>\n",
       "      <td>249200000.0</td>\n",
       "      <td>25569</td>\n",
       "      <td>ELMD</td>\n",
       "      <td>ELMD3</td>\n",
       "      <td>BRELMDACNOR3</td>\n",
       "      <td>CIA NOVO MERCADO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91026</th>\n",
       "      <td>129891</td>\n",
       "      <td>COMUNICACOES</td>\n",
       "      <td>TELECOMUNICACOES</td>\n",
       "      <td>TELECOMUNICACOES</td>\n",
       "      <td>UNIFIQUE TELECOMUNICACOES SA</td>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>1</td>\n",
       "      <td>Cota√ß√µes Hist√≥ricas</td>\n",
       "      <td>A√ß√µes Ordin√°rias (ON)</td>\n",
       "      <td>99.3</td>\n",
       "      <td>Cota√ß√£o Mediana do Trimestre</td>\n",
       "      <td>3.278778</td>\n",
       "      <td>26050</td>\n",
       "      <td>FIQE</td>\n",
       "      <td>FIQE3</td>\n",
       "      <td>BRFIQEACNOR5</td>\n",
       "      <td>CIA NOVO MERCADO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91027</th>\n",
       "      <td>131935</td>\n",
       "      <td>COMUNICACOES</td>\n",
       "      <td>TELECOMUNICACOES</td>\n",
       "      <td>TELECOMUNICACOES</td>\n",
       "      <td>UNIFIQUE TELECOMUNICACOES SA</td>\n",
       "      <td>2023-09-30</td>\n",
       "      <td>1</td>\n",
       "      <td>Cota√ß√µes Hist√≥ricas</td>\n",
       "      <td>A√ß√µes Ordin√°rias (ON)</td>\n",
       "      <td>99.3</td>\n",
       "      <td>Cota√ß√£o Mediana do Trimestre</td>\n",
       "      <td>3.719842</td>\n",
       "      <td>26050</td>\n",
       "      <td>FIQE</td>\n",
       "      <td>FIQE3</td>\n",
       "      <td>BRFIQEACNOR5</td>\n",
       "      <td>CIA NOVO MERCADO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91028</th>\n",
       "      <td>134906</td>\n",
       "      <td>COMUNICACOES</td>\n",
       "      <td>TELECOMUNICACOES</td>\n",
       "      <td>TELECOMUNICACOES</td>\n",
       "      <td>UNIFIQUE TELECOMUNICACOES SA</td>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>1</td>\n",
       "      <td>Cota√ß√µes Hist√≥ricas</td>\n",
       "      <td>A√ß√µes Ordin√°rias (ON)</td>\n",
       "      <td>99.3</td>\n",
       "      <td>Cota√ß√£o Mediana do Trimestre</td>\n",
       "      <td>3.490817</td>\n",
       "      <td>26050</td>\n",
       "      <td>FIQE</td>\n",
       "      <td>FIQE3</td>\n",
       "      <td>BRFIQEACNOR5</td>\n",
       "      <td>CIA NOVO MERCADO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91029</th>\n",
       "      <td>137478</td>\n",
       "      <td>COMUNICACOES</td>\n",
       "      <td>TELECOMUNICACOES</td>\n",
       "      <td>TELECOMUNICACOES</td>\n",
       "      <td>UNIFIQUE TELECOMUNICACOES SA</td>\n",
       "      <td>2024-03-31</td>\n",
       "      <td>1</td>\n",
       "      <td>Cota√ß√µes Hist√≥ricas</td>\n",
       "      <td>A√ß√µes Ordin√°rias (ON)</td>\n",
       "      <td>99.3</td>\n",
       "      <td>Cota√ß√£o Mediana do Trimestre</td>\n",
       "      <td>3.726683</td>\n",
       "      <td>26050</td>\n",
       "      <td>FIQE</td>\n",
       "      <td>FIQE3</td>\n",
       "      <td>BRFIQEACNOR5</td>\n",
       "      <td>CIA NOVO MERCADO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91030</th>\n",
       "      <td>140491</td>\n",
       "      <td>COMUNICACOES</td>\n",
       "      <td>TELECOMUNICACOES</td>\n",
       "      <td>TELECOMUNICACOES</td>\n",
       "      <td>UNIFIQUE TELECOMUNICACOES SA</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>1</td>\n",
       "      <td>Cota√ß√µes Hist√≥ricas</td>\n",
       "      <td>A√ß√µes Ordin√°rias (ON)</td>\n",
       "      <td>99.3</td>\n",
       "      <td>Cota√ß√£o Mediana do Trimestre</td>\n",
       "      <td>3.544215</td>\n",
       "      <td>26050</td>\n",
       "      <td>FIQE</td>\n",
       "      <td>FIQE3</td>\n",
       "      <td>BRFIQEACNOR5</td>\n",
       "      <td>CIA NOVO MERCADO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91031 rows √ó 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          nsd        sector         subsector                   segment  \\\n",
       "0       99116  COMUNICACOES             MIDIA  PUBLICIDADE E PROPAGANDA   \n",
       "1       99116  COMUNICACOES             MIDIA  PUBLICIDADE E PROPAGANDA   \n",
       "2       99116  COMUNICACOES             MIDIA  PUBLICIDADE E PROPAGANDA   \n",
       "3       99116  COMUNICACOES             MIDIA  PUBLICIDADE E PROPAGANDA   \n",
       "4       99116  COMUNICACOES             MIDIA  PUBLICIDADE E PROPAGANDA   \n",
       "...       ...           ...               ...                       ...   \n",
       "91026  129891  COMUNICACOES  TELECOMUNICACOES          TELECOMUNICACOES   \n",
       "91027  131935  COMUNICACOES  TELECOMUNICACOES          TELECOMUNICACOES   \n",
       "91028  134906  COMUNICACOES  TELECOMUNICACOES          TELECOMUNICACOES   \n",
       "91029  137478  COMUNICACOES  TELECOMUNICACOES          TELECOMUNICACOES   \n",
       "91030  140491  COMUNICACOES  TELECOMUNICACOES          TELECOMUNICACOES   \n",
       "\n",
       "                       company_name    quarter version                 type  \\\n",
       "0                    ELETROMIDIA SA 2019-12-31       1     DFs Consolidadas   \n",
       "1                    ELETROMIDIA SA 2019-12-31       1     DFs Consolidadas   \n",
       "2                    ELETROMIDIA SA 2019-12-31       1     DFs Consolidadas   \n",
       "3                    ELETROMIDIA SA 2019-12-31       1     DFs Consolidadas   \n",
       "4                    ELETROMIDIA SA 2019-12-31       1     DFs Consolidadas   \n",
       "...                             ...        ...     ...                  ...   \n",
       "91026  UNIFIQUE TELECOMUNICACOES SA 2023-06-30       1  Cota√ß√µes Hist√≥ricas   \n",
       "91027  UNIFIQUE TELECOMUNICACOES SA 2023-09-30       1  Cota√ß√µes Hist√≥ricas   \n",
       "91028  UNIFIQUE TELECOMUNICACOES SA 2023-12-31       1  Cota√ß√µes Hist√≥ricas   \n",
       "91029  UNIFIQUE TELECOMUNICACOES SA 2024-03-31       1  Cota√ß√µes Hist√≥ricas   \n",
       "91030  UNIFIQUE TELECOMUNICACOES SA 2024-06-30       1  Cota√ß√µes Hist√≥ricas   \n",
       "\n",
       "                           frame      account  \\\n",
       "0      Balan√ßo Patrimonial Ativo           01   \n",
       "1      Balan√ßo Patrimonial Ativo        01.01   \n",
       "2      Balan√ßo Patrimonial Ativo     01.01.01   \n",
       "3      Balan√ßo Patrimonial Ativo  01.01.01.01   \n",
       "4      Balan√ßo Patrimonial Ativo  01.01.01.02   \n",
       "...                          ...          ...   \n",
       "91026      A√ß√µes Ordin√°rias (ON)         99.3   \n",
       "91027      A√ß√µes Ordin√°rias (ON)         99.3   \n",
       "91028      A√ß√µes Ordin√°rias (ON)         99.3   \n",
       "91029      A√ß√µes Ordin√°rias (ON)         99.3   \n",
       "91030      A√ß√µes Ordin√°rias (ON)         99.3   \n",
       "\n",
       "                                        description         value cvm_code  \\\n",
       "0                                       Ativo Total  2905300000.0    25569   \n",
       "1                   Ativo Circulante de Curto Prazo  1061320000.0    25569   \n",
       "2      Caixa e Equivalentes de Caixa de Curto Prazo   380180000.0    25569   \n",
       "3                     Caixa e Bancos de Curto Prazo   130980000.0    25569   \n",
       "4                Aplica√ß√µes L√≠quidas de Curto Prazo   249200000.0    25569   \n",
       "...                                             ...           ...      ...   \n",
       "91026                  Cota√ß√£o Mediana do Trimestre      3.278778    26050   \n",
       "91027                  Cota√ß√£o Mediana do Trimestre      3.719842    26050   \n",
       "91028                  Cota√ß√£o Mediana do Trimestre      3.490817    26050   \n",
       "91029                  Cota√ß√£o Mediana do Trimestre      3.726683    26050   \n",
       "91030                  Cota√ß√£o Mediana do Trimestre      3.544215    26050   \n",
       "\n",
       "      ticker ticker_codes    isin_codes           listing  \n",
       "0       ELMD        ELMD3  BRELMDACNOR3  CIA NOVO MERCADO  \n",
       "1       ELMD        ELMD3  BRELMDACNOR3  CIA NOVO MERCADO  \n",
       "2       ELMD        ELMD3  BRELMDACNOR3  CIA NOVO MERCADO  \n",
       "3       ELMD        ELMD3  BRELMDACNOR3  CIA NOVO MERCADO  \n",
       "4       ELMD        ELMD3  BRELMDACNOR3  CIA NOVO MERCADO  \n",
       "...      ...          ...           ...               ...  \n",
       "91026   FIQE        FIQE3  BRFIQEACNOR5  CIA NOVO MERCADO  \n",
       "91027   FIQE        FIQE3  BRFIQEACNOR5  CIA NOVO MERCADO  \n",
       "91028   FIQE        FIQE3  BRFIQEACNOR5  CIA NOVO MERCADO  \n",
       "91029   FIQE        FIQE3  BRFIQEACNOR5  CIA NOVO MERCADO  \n",
       "91030   FIQE        FIQE3  BRFIQEACNOR5  CIA NOVO MERCADO  \n",
       "\n",
       "[91031 rows x 17 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert new rows into df_statements_companies\n",
    "\n",
    "df_final = pd.concat(\n",
    "    [df_statements_companies, pd.DataFrame(new_rows)], ignore_index=True\n",
    ").drop_duplicates()\n",
    "\n",
    "mask = df_final[\"ticker\"] == \"ELMD\"\n",
    "# mask&= df_historical_statements_2['quarter'] == '2020-12-31'\n",
    "mask &= df_final[\"type\"] == \"Cota√ß√µes Hist√≥ricas\"\n",
    "\n",
    "df_final\n",
    "\n",
    "# sort and select only statement columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finantial Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector = \"COMUNICACOES\"\n",
    "db_file = f\"..\\data\\{settings.db_filepath.split('.')[0]} {settings.markets_file}.db\"\n",
    "\n",
    "conn = sqlite3.connect(db_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Fetch all table names excluding internal SQLite tables\n",
    "cursor.execute(\n",
    "    \"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';\"\n",
    ")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "df = pd.read_sql_query(f\"SELECT * FROM {sector}\", conn)\n",
    "# df['account_description'] = df['account'] + ' - ' + df['description']\n",
    "df[\"quarter\"] = pd.to_datetime(df[\"quarter\"])\n",
    "# Store unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sector': ['COMUNICACOES'],\n",
       " 'subsector': ['MIDIA', 'TELECOMUNICACOES'],\n",
       " 'segment': ['PUBLICIDADE E PROPAGANDA', 'TELECOMUNICACOES'],\n",
       " 'company_name': ['ALGAR TELECOM SA',\n",
       "  'BRISANET PARTICIPACOES SA',\n",
       "  'DESKTOP SA',\n",
       "  'ELETROMIDIA SA',\n",
       "  'GIGA MAIS FIBRA TELECOMUNICACOES SA',\n",
       "  'OI SA',\n",
       "  'TELEC BRASILEIRAS SA TELEBRAS',\n",
       "  'TELEFONICA BRASIL SA',\n",
       "  'TIM SA',\n",
       "  'UNIFIQUE TELECOMUNICACOES SA'],\n",
       " 'quarter': [Timestamp('2010-12-31 00:00:00'),\n",
       "  Timestamp('2011-03-31 00:00:00'),\n",
       "  Timestamp('2011-06-30 00:00:00'),\n",
       "  Timestamp('2011-09-30 00:00:00'),\n",
       "  Timestamp('2011-12-31 00:00:00'),\n",
       "  Timestamp('2012-03-31 00:00:00'),\n",
       "  Timestamp('2012-06-30 00:00:00'),\n",
       "  Timestamp('2012-09-30 00:00:00'),\n",
       "  Timestamp('2012-12-31 00:00:00'),\n",
       "  Timestamp('2013-03-31 00:00:00'),\n",
       "  Timestamp('2013-06-30 00:00:00'),\n",
       "  Timestamp('2013-09-30 00:00:00'),\n",
       "  Timestamp('2013-12-31 00:00:00'),\n",
       "  Timestamp('2014-03-31 00:00:00'),\n",
       "  Timestamp('2014-06-30 00:00:00'),\n",
       "  Timestamp('2014-09-30 00:00:00'),\n",
       "  Timestamp('2014-12-31 00:00:00'),\n",
       "  Timestamp('2015-03-31 00:00:00'),\n",
       "  Timestamp('2015-06-30 00:00:00'),\n",
       "  Timestamp('2015-09-30 00:00:00'),\n",
       "  Timestamp('2015-12-31 00:00:00'),\n",
       "  Timestamp('2016-03-31 00:00:00'),\n",
       "  Timestamp('2016-06-30 00:00:00'),\n",
       "  Timestamp('2016-09-30 00:00:00'),\n",
       "  Timestamp('2016-12-31 00:00:00'),\n",
       "  Timestamp('2017-03-31 00:00:00'),\n",
       "  Timestamp('2017-06-30 00:00:00'),\n",
       "  Timestamp('2017-09-30 00:00:00'),\n",
       "  Timestamp('2017-12-31 00:00:00'),\n",
       "  Timestamp('2018-03-31 00:00:00'),\n",
       "  Timestamp('2018-06-30 00:00:00'),\n",
       "  Timestamp('2018-09-30 00:00:00'),\n",
       "  Timestamp('2018-12-31 00:00:00'),\n",
       "  Timestamp('2019-03-31 00:00:00'),\n",
       "  Timestamp('2019-06-30 00:00:00'),\n",
       "  Timestamp('2019-09-30 00:00:00'),\n",
       "  Timestamp('2019-12-31 00:00:00'),\n",
       "  Timestamp('2020-03-31 00:00:00'),\n",
       "  Timestamp('2020-06-30 00:00:00'),\n",
       "  Timestamp('2020-09-30 00:00:00'),\n",
       "  Timestamp('2020-12-31 00:00:00'),\n",
       "  Timestamp('2021-03-31 00:00:00'),\n",
       "  Timestamp('2021-06-30 00:00:00'),\n",
       "  Timestamp('2021-09-30 00:00:00'),\n",
       "  Timestamp('2021-12-31 00:00:00'),\n",
       "  Timestamp('2022-03-31 00:00:00'),\n",
       "  Timestamp('2022-06-30 00:00:00'),\n",
       "  Timestamp('2022-09-30 00:00:00'),\n",
       "  Timestamp('2022-12-31 00:00:00'),\n",
       "  Timestamp('2023-03-31 00:00:00'),\n",
       "  Timestamp('2023-06-30 00:00:00'),\n",
       "  Timestamp('2023-09-30 00:00:00'),\n",
       "  Timestamp('2023-12-31 00:00:00'),\n",
       "  Timestamp('2024-03-31 00:00:00'),\n",
       "  Timestamp('2024-06-30 00:00:00')],\n",
       " 'version': ['1', '2', '3', '4'],\n",
       " 'type': ['Cota√ß√µes Hist√≥ricas',\n",
       "  'DFs Consolidadas',\n",
       "  'DFs Individuais',\n",
       "  'Dados da Empresa'],\n",
       " 'frame': ['Balan√ßo Patrimonial Ativo',\n",
       "  'Balan√ßo Patrimonial Passivo',\n",
       "  'Composi√ß√£o do Capital',\n",
       "  'Cota√ß√£o Mediana do Trimestre',\n",
       "  'Demonstra√ß√£o de Valor Adicionado',\n",
       "  'Demonstra√ß√£o do Fluxo de Caixa',\n",
       "  'Demonstra√ß√£o do Resultado'],\n",
       " 'account': ['00.01.01',\n",
       "  '00.01.02',\n",
       "  '00.02.01',\n",
       "  '00.02.02',\n",
       "  '01',\n",
       "  '01.01',\n",
       "  '01.01.01',\n",
       "  '01.01.01.01',\n",
       "  '01.01.01.02',\n",
       "  '01.01.02',\n",
       "  '01.01.02.01',\n",
       "  '01.01.02.02',\n",
       "  '01.01.03',\n",
       "  '01.01.03.01',\n",
       "  '01.01.03.01.01',\n",
       "  '01.01.03.01.02',\n",
       "  '01.01.03.01.03',\n",
       "  '01.01.03.02',\n",
       "  '01.01.04',\n",
       "  '01.01.04.01',\n",
       "  '01.01.04.02',\n",
       "  '01.01.04.03',\n",
       "  '01.01.05',\n",
       "  '01.01.06',\n",
       "  '01.01.07',\n",
       "  '01.01.09',\n",
       "  '01.02',\n",
       "  '01.02.01',\n",
       "  '01.02.01.02',\n",
       "  '01.02.01.03',\n",
       "  '01.02.01.04',\n",
       "  '01.02.01.05',\n",
       "  '01.02.01.06',\n",
       "  '01.02.01.07',\n",
       "  '01.02.01.08',\n",
       "  '01.02.01.09',\n",
       "  '01.02.02',\n",
       "  '01.02.02.01',\n",
       "  '01.02.02.01.01',\n",
       "  '01.02.02.01.02',\n",
       "  '01.02.02.01.03',\n",
       "  '01.02.02.02',\n",
       "  '01.02.03',\n",
       "  '01.02.03.01',\n",
       "  '01.02.03.02',\n",
       "  '01.02.04',\n",
       "  '01.02.04.01',\n",
       "  '01.02.04.01.01',\n",
       "  '01.02.04.01.02',\n",
       "  '01.02.04.01.03',\n",
       "  '01.02.04.02',\n",
       "  '01.02.04.02.01',\n",
       "  '02',\n",
       "  '02.01',\n",
       "  '02.01.01.01',\n",
       "  '02.01.01.02',\n",
       "  '02.01.02',\n",
       "  '02.01.02.01',\n",
       "  '02.01.02.02',\n",
       "  '02.01.03',\n",
       "  '02.01.03.01',\n",
       "  '02.01.03.01.01',\n",
       "  '02.01.03.01.02',\n",
       "  '02.01.03.01.03',\n",
       "  '02.01.03.02',\n",
       "  '02.01.03.03',\n",
       "  '02.01.04',\n",
       "  '02.01.04.01',\n",
       "  '02.01.04.01.01',\n",
       "  '02.01.04.01.02',\n",
       "  '02.01.04.02',\n",
       "  '02.01.04.03',\n",
       "  '02.01.05',\n",
       "  '02.01.05.01',\n",
       "  '02.01.05.01.01',\n",
       "  '02.01.05.01.03',\n",
       "  '02.01.05.01.04',\n",
       "  '02.01.05.02',\n",
       "  '02.01.05.02.01',\n",
       "  '02.01.05.02.02',\n",
       "  '02.01.05.02.03',\n",
       "  '02.01.05.02.04',\n",
       "  '02.01.05.02.09',\n",
       "  '02.01.06',\n",
       "  '02.01.06.01',\n",
       "  '02.01.06.01.01',\n",
       "  '02.01.06.01.02',\n",
       "  '02.01.06.01.03',\n",
       "  '02.01.06.01.04',\n",
       "  '02.01.06.02',\n",
       "  '02.01.06.02.01',\n",
       "  '02.01.06.02.02',\n",
       "  '02.01.06.02.03',\n",
       "  '02.02',\n",
       "  '02.02.01',\n",
       "  '02.02.01.01',\n",
       "  '02.02.01.01.01',\n",
       "  '02.02.01.01.02',\n",
       "  '02.02.01.02',\n",
       "  '02.02.01.03',\n",
       "  '02.02.02',\n",
       "  '02.02.02.01.04',\n",
       "  '02.02.03',\n",
       "  '02.02.04',\n",
       "  '02.02.04.01',\n",
       "  '02.03',\n",
       "  '02.03.01',\n",
       "  '02.03.01.01',\n",
       "  '02.03.01.02',\n",
       "  '02.03.02',\n",
       "  '02.03.02.01',\n",
       "  '02.03.02.02',\n",
       "  '02.03.02.09',\n",
       "  '02.03.03',\n",
       "  '02.03.04',\n",
       "  '02.03.04.01',\n",
       "  '02.03.04.02',\n",
       "  '02.03.04.03',\n",
       "  '02.03.04.09',\n",
       "  '02.03.05',\n",
       "  '02.03.06',\n",
       "  '02.03.06.01',\n",
       "  '02.03.06.02',\n",
       "  '02.03.06.09',\n",
       "  '02.03.07',\n",
       "  '02.03.08',\n",
       "  '02.03.09',\n",
       "  '03.01',\n",
       "  '03.02',\n",
       "  '03.03',\n",
       "  '03.04',\n",
       "  '03.04.01',\n",
       "  '03.04.01.01',\n",
       "  '03.05',\n",
       "  '03.06',\n",
       "  '03.07',\n",
       "  '03.08',\n",
       "  '03.11',\n",
       "  '03.11.01',\n",
       "  '03.11.02',\n",
       "  '06.01',\n",
       "  '06.02',\n",
       "  '06.03',\n",
       "  '07.01',\n",
       "  '07.01.01',\n",
       "  '07.01.02',\n",
       "  '07.01.03',\n",
       "  '07.01.04',\n",
       "  '07.02',\n",
       "  '07.02.01',\n",
       "  '07.02.02',\n",
       "  '07.02.03',\n",
       "  '07.02.04',\n",
       "  '07.03',\n",
       "  '07.04',\n",
       "  '07.04.01',\n",
       "  '07.04.02',\n",
       "  '07.05',\n",
       "  '07.06',\n",
       "  '07.06.01',\n",
       "  '07.06.02',\n",
       "  '07.06.03',\n",
       "  '07.06.03.01',\n",
       "  '07.06.03.02',\n",
       "  '07.07',\n",
       "  '07.08',\n",
       "  '07.08.01',\n",
       "  '07.08.01.01',\n",
       "  '07.08.01.02',\n",
       "  '07.08.01.03',\n",
       "  '07.08.01.04',\n",
       "  '07.08.02',\n",
       "  '07.08.02.01',\n",
       "  '07.08.02.02',\n",
       "  '07.08.02.03',\n",
       "  '07.08.03',\n",
       "  '07.08.03.01',\n",
       "  '07.08.03.02',\n",
       "  '07.08.03.03',\n",
       "  '07.08.04',\n",
       "  '07.08.04.01',\n",
       "  '07.08.04.02',\n",
       "  '07.08.04.03',\n",
       "  '07.08.04.04',\n",
       "  '07.08.05.01',\n",
       "  '07.08.05.02',\n",
       "  '07.08.05.03',\n",
       "  '07.08.05.04',\n",
       "  '07.08.05.09',\n",
       "  '99.3',\n",
       "  '99.4'],\n",
       " 'description': ['Ajustes Acumulados de Convers√£o',\n",
       "  'Ajustes Patrimoniais',\n",
       "  'Ajustes de Avalia√ß√£o Patrimonial',\n",
       "  'Alugu√©is',\n",
       "  'Aplica√ß√µes Financeiras de Curto Prazo',\n",
       "  'Aplica√ß√µes L√≠quidas de Curto Prazo',\n",
       "  'Aplica√ß√µes a Valor Justo de Curto Prazo',\n",
       "  'Aplica√ß√µes a Valor Justo de Longo Prazo',\n",
       "  'Aplica√ß√µes ao Custo Amortizado de Curto Prazo',\n",
       "  'Ativo Circulante de Curto Prazo',\n",
       "  'Ativo N√£o Circulante de Longo Prazo',\n",
       "  'Ativo Realiz√°vel a Longo Prazo',\n",
       "  'Ativo Total',\n",
       "  'Ativos Biol√≥gicos de Curto Prazo',\n",
       "  'Ativos Biol√≥gicos de Longo Prazo',\n",
       "  'Atribu√≠do a S√≥cios N√£o Controladores',\n",
       "  'Atribu√≠do a S√≥cios da Empresa Controladora',\n",
       "  'A√ß√µes ON Ordin√°rias',\n",
       "  'A√ß√µes Ordin√°rias (ON)',\n",
       "  'A√ß√µes PN Preferenciais',\n",
       "  'A√ß√µes Preferenciais (PN)',\n",
       "  'A√ß√µes, Remunera√ß√£o e Op√ß√µes',\n",
       "  'Benef√≠cios',\n",
       "  'Caixa de Financiamento',\n",
       "  'Caixa de Investimento',\n",
       "  'Caixa de Opera√ß√µes (Operacional)',\n",
       "  'Caixa e Bancos de Curto Prazo',\n",
       "  'Caixa e Equivalentes de Caixa de Curto Prazo',\n",
       "  'Capital Social',\n",
       "  'Capital Social Realizado',\n",
       "  'Carteira de Clientes',\n",
       "  'Clientes',\n",
       "  'Contas a Receber de Curto Prazo',\n",
       "  'Contas a Receber de Longo Prazo',\n",
       "  'Contas de Clientes de Curto Prazo',\n",
       "  'Cr√©ditos com Partes Relacionadas de Longo Prazo',\n",
       "  'Cr√©ditos de Liquida√ß√£o Duvidosa',\n",
       "  'Custo dos Bens e/ou Servi√ßos Vendidos',\n",
       "  'Custos Prods., Mercs. e Servs. Vendidos',\n",
       "  'Deb√™ntures',\n",
       "  'Deprecia√ß√£o, Amortiza√ß√£o e Exaust√£o',\n",
       "  'Derivativos e Participa√ß√µes',\n",
       "  'Despesas Antecipadas de Curto Prazo',\n",
       "  'Despesas Antecipadas de Longo Prazo',\n",
       "  'Despesas Comerciais',\n",
       "  'Despesas Operacionais',\n",
       "  'Despesas/Receitas Operacionais',\n",
       "  'Direito de Uso em Arrendamento',\n",
       "  'Distribui√ß√£o do Valor Adicionado',\n",
       "  'Dividendos',\n",
       "  'Dividendos e A√ß√µes',\n",
       "  'Dividendos e A√ß√µes em Tesouraria',\n",
       "  'D√©bitos com Coligadas',\n",
       "  'D√©bitos com Controladores',\n",
       "  'D√©bitos com Outras Partes Relacionadas',\n",
       "  'Em Moeda Estrangeira',\n",
       "  'Em Moeda Nacional',\n",
       "  'Em Tesouraria A√ß√µes ON Ordin√°rias',\n",
       "  'Em Tesouraria A√ß√µes PN Preferenciais',\n",
       "  'Empr√©stimos e Financiamentos',\n",
       "  'Empr√©stimos e Financiamentos de Curto Prazo',\n",
       "  'Empr√©stimos e Financiamentos de Longo Prazo',\n",
       "  'Estaduais',\n",
       "  'Estoques de Curto Prazo',\n",
       "  'Estoques de Longo Prazo',\n",
       "  'Estoques de Material de Consumo de Curto Prazo',\n",
       "  'Estoques de Material para Revenda de Curto Prazo',\n",
       "  'Estoques de Outros Itens de Curto Prazo',\n",
       "  'F.G.T.S.',\n",
       "  'Federais',\n",
       "  'Financiamento por Arrendamento Financeiro',\n",
       "  'Fornecedores Estrangeiros',\n",
       "  'Fornecedores Nacionais',\n",
       "  'Fornecedores de Curto Prazo',\n",
       "  'Gastos na emiss√£o de a√ß√µes',\n",
       "  'Goodwill',\n",
       "  'Imobilizado',\n",
       "  'Imobilizado em Opera√ß√£o',\n",
       "  'Imposto de Renda e Contribui√ß√£o Social Diferidos',\n",
       "  'Imposto de Renda e Contribui√ß√£o Social a Pagar',\n",
       "  'Imposto de Renda e Contribui√ß√£o Social sobre o Lucro',\n",
       "  'Impostos, Taxas e Contribui√ß√µes',\n",
       "  'Insumos Adquiridos de Terceiros',\n",
       "  'Intang√≠veis',\n",
       "  'Intang√≠vel',\n",
       "  'Investimento Social',\n",
       "  'Investimentos',\n",
       "  'Juros',\n",
       "  'Juros sobre o Capital Pr√≥prio',\n",
       "  'Lucro do Per√≠odo',\n",
       "  'Lucros Retidos',\n",
       "  'Lucros Retidos / Preju√≠zo do Per√≠odo',\n",
       "  'Lucros/Preju√≠zos Acumulados',\n",
       "  'Marcas e Patentes',\n",
       "  'Materiais, Energia, Servs. de Terceiros e Outros',\n",
       "  'Municipais',\n",
       "  'Obriga√ß√µes Fiscais Estaduais',\n",
       "  'Obriga√ß√µes Fiscais Federais',\n",
       "  'Obriga√ß√µes Fiscais Municipais',\n",
       "  'Obriga√ß√µes Fiscais de Curto Prazo',\n",
       "  'Obriga√ß√µes Sociais',\n",
       "  'Obriga√ß√µes Trabalhistas',\n",
       "  'Obriga√ß√µes Tribut√°rias e Autoriza√ß√µes',\n",
       "  'Outras',\n",
       "  'Outras Contas de Curto Prazo',\n",
       "  'Outras Obriga√ß√µes Fiscais Federais',\n",
       "  'Outras Obriga√ß√µes de Curto Prazo',\n",
       "  'Outras Provis√µes',\n",
       "  'Outras Receitas',\n",
       "  'Outros',\n",
       "  'Outros Ativos Circulantes de Curto Prazo',\n",
       "  'Outros Ativos Circulantes de Longo Prazo',\n",
       "  'Outros Resultados Abrangentes',\n",
       "  'Part. N√£o Controladores nos Lucros Retidos',\n",
       "  'Participa√ß√£o Minorit√°ria',\n",
       "  'Participa√ß√£o dos Acionistas N√£o Controladores',\n",
       "  'Participa√ß√µes Societ√°rias',\n",
       "  'Participa√ß√µes em Coligadas',\n",
       "  'Participa√ß√µes em Controladas',\n",
       "  'Passivo Circulante de Curto Prazo',\n",
       "  'Passivo N√£o Circulante de Longo Prazo',\n",
       "  'Passivo Total',\n",
       "  'Passivos com Partes Relacionadas',\n",
       "  'Passivos com Partes Relacionadas de Longo Prazo',\n",
       "  'Patrim√¥nio L√≠quido',\n",
       "  'Perda/Recupera√ß√£o de Valores Ativos',\n",
       "  'Perdas e Aquisi√ß√µes com N√£o Controladores',\n",
       "  'Pessoal',\n",
       "  'Propriedades para Investimento',\n",
       "  'Provis√£o/Revers√£o de Cr√©ds. Liquida√ß√£o Duvidosa',\n",
       "  'Provis√µes C√≠veis',\n",
       "  'Provis√µes Fiscais',\n",
       "  'Provis√µes Fiscais Previdenci√°rias Trabalhistas e C√≠veis',\n",
       "  'Provis√µes Judiciais',\n",
       "  'Provis√µes Previdenci√°rias e Trabalhistas',\n",
       "  'Provis√µes de Curto Prazo',\n",
       "  'Provis√µes de Longo Prazo',\n",
       "  'Provis√µes para Benef√≠cios a Empregados',\n",
       "  'Provis√µes para Garantias',\n",
       "  'Provis√µes para Passivos Ambientais e de Desativa√ß√£o',\n",
       "  'Provis√µes para Reestrutura√ß√£o',\n",
       "  'Provis√µes trabalhistas e c√≠veis, l√≠quidas',\n",
       "  'Receita de Venda de Bens e/ou Servi√ßos',\n",
       "  'Receitas',\n",
       "  'Receitas Financeiras',\n",
       "  'Receitas refs. √† Constru√ß√£o de Ativos Pr√≥prios',\n",
       "  'Remunera√ß√£o Direta',\n",
       "  'Remunera√ß√£o de Capitais Pr√≥prios',\n",
       "  'Remunera√ß√£o de Capitais de Terceiros',\n",
       "  'Reservas Legais e Estatut√°rias',\n",
       "  'Reservas de Capital',\n",
       "  'Reservas de Lucros',\n",
       "  'Reservas de Reavalia√ß√£o',\n",
       "  'Resultado Antes do Resultado Financeiro e dos Tributos',\n",
       "  'Resultado Antes dos Tributos sobre o Lucro',\n",
       "  'Resultado Bruto',\n",
       "  'Resultado Financeiro',\n",
       "  'Resultado de Equival√™ncia Patrimonial',\n",
       "  'Reten√ß√£o de Lucros e Incentivos Fiscais',\n",
       "  'Reten√ß√µes',\n",
       "  'Softwares',\n",
       "  'Telecomunica√ß√µes e Consigna√ß√µes',\n",
       "  'Tributos Parcelados',\n",
       "  'Tributos a Recuperar de Curto Prazo',\n",
       "  'Tributos a Recuperar de Longo Prazo',\n",
       "  'Valor Adicionado Bruto',\n",
       "  'Valor Adicionado L√≠quido Produzido',\n",
       "  'Valor Adicionado Total a Distribuir',\n",
       "  'Vendas de Mercadorias, Produtos e Servi√ßos',\n",
       "  'Vlr Adicionado Recebido em Transfer√™ncia',\n",
       "  '√Ågio e Reserva Especial']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uv = {}\n",
    "for col in df.columns:\n",
    "    if col not in [\"nsd\", \"value\"]:\n",
    "        uv[col] = sorted(df[col].unique().tolist())\n",
    "uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bovespa Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_company_data(url, base_url=\"https://bvmf.bmfbovespa.com.br/sig/\"):\n",
    "    response = requests.get(url, headers=header_random(), verify=False)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find the relevant table\n",
    "        table = soup.find(\"table\", {\"width\": \"95%\"})\n",
    "\n",
    "        # Initialize lists to store data\n",
    "        company_name = []\n",
    "        trading_name = []\n",
    "        listing = []\n",
    "        ticker = []\n",
    "        # url_mercado = []\n",
    "\n",
    "        if table:\n",
    "            # Iterate through rows of the table and extract data\n",
    "            for row in table.find_all(\"tr\")[1:]:  # Skip the header\n",
    "                columns = row.find_all(\"td\")\n",
    "\n",
    "                # Check if the row has enough columns\n",
    "                if len(columns) >= 4:\n",
    "                    company_name.append(\n",
    "                        clean_text(columns[0].text.strip())\n",
    "                    )  # Raz√£o Social\n",
    "                    trading_name.append(\n",
    "                        clean_text(columns[1].text.strip())\n",
    "                    )  # Nome de Preg√£o\n",
    "                    listing.append(\n",
    "                        clean_text(columns[2].text.strip())\n",
    "                    )  # Tipo de Mercado\n",
    "                    ticker.append(clean_text(columns[3].text.strip()))  # Sigla\n",
    "\n",
    "                    # # Extract the link\n",
    "                    # link = columns[0].find('a')['href']\n",
    "                    # full_link = f\"{base_url}{link}\"\n",
    "                    # url_mercado.append(full_link)\n",
    "\n",
    "        # Return the extracted data as a DataFrame\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"company_name\": company_name,\n",
    "                \"trading_name\": trading_name,\n",
    "                \"ticker\": ticker,\n",
    "                \"listing\": listing,\n",
    "                # 'url_mercado': url_mercado\n",
    "            }\n",
    "        )\n",
    "        df[\"listing\"] = (\n",
    "            df[\"listing\"].map(settings.governance_levels).fillna(df[\"listing\"])\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_companies_listing():\n",
    "    try:\n",
    "        from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "        # Ignorar avisos de SSL (opcional)\n",
    "        requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "        # List of characters A-Z and 0-9\n",
    "        chars = list(string.digits) + list(string.ascii_uppercase)\n",
    "\n",
    "        # Generate URLs based on each character\n",
    "        base_urls = [\n",
    "            f\"https://bvmf.bmfbovespa.com.br/sig/FormConsultaEmpResultado.asp?strLetraInicial={char}\"\n",
    "            for char in chars\n",
    "        ]\n",
    "\n",
    "        # Initialize an empty DataFrame to store all companies\n",
    "        dfs = []\n",
    "\n",
    "        # Iterate through each URL and fetch the data\n",
    "        start_time = time.time()\n",
    "        for i, url in enumerate(base_urls):\n",
    "            # companies from url\n",
    "            df = parse_company_data(url)\n",
    "            dfs.append(df)\n",
    "\n",
    "            extra_info = [url, f\"{len(df)} companies listed\"]\n",
    "            print_info(i, extra_info, start_time, len(base_urls))\n",
    "\n",
    "        # DataFrame with all companies\n",
    "        companies = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        companies = pd.DataFrame()\n",
    "\n",
    "    return companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_stock_info(soup):\n",
    "    stock_info = {}\n",
    "\n",
    "    # Locate the rows with \"Nome da A√ß√£o\" to get stock name and ticker\n",
    "    stock_rows = soup.find_all(\"td\", class_=\"tittabela\", colspan=\"7\")\n",
    "\n",
    "    for row in stock_rows:\n",
    "        stock_text = row.get_text(strip=True)\n",
    "        if \"Nome da A√ß√£o\" in stock_text:\n",
    "            # Extract the stock name and ticker (in parentheses)\n",
    "            stock_name = stock_text.split(\"(\")[0].replace(\"Nome da A√ß√£o:\", \"\").strip()\n",
    "            stock_ticker = stock_text.split(\"(\")[1].replace(\")\", \"\").strip()\n",
    "\n",
    "            # Add stock_name and stock_ticker to the dictionary\n",
    "            stock_info[stock_name] = stock_ticker\n",
    "    return stock_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages_to_download(companies, start_date):\n",
    "    tipo = \"RES_MERC_VISTA\"\n",
    "\n",
    "    date_range = [\n",
    "        date.strftime(\"%m-%Y\")\n",
    "        for date in pd.date_range(\n",
    "            start=start_date,\n",
    "            end=pd.Timestamp.today() - pd.offsets.MonthEnd(1),\n",
    "            freq=\"MS\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Generate the items list with ticker, company_name, trading_name, and date\n",
    "    items = [\n",
    "        [row[\"ticker\"], row[\"company_name\"], row[\"trading_name\"], mes]\n",
    "        for _, row in companies.iterrows()\n",
    "        for mes in date_range\n",
    "    ]\n",
    "\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert text with '.' as thousand separator and ',' as decimal separator\n",
    "def convert_to_float(value):\n",
    "    try:\n",
    "        # Remove thousand separator and replace decimal separator\n",
    "        return float(value.replace(\".\", \"\").replace(\",\", \".\"))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def convert_to_int(value):\n",
    "    try:\n",
    "        # Remove thousand separator and convert to int\n",
    "        return int(value.replace(\".\", \"\").replace(\",\", \"\"))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_trades(soup, stock_info, ticker_code, company_name, trading_name, date):\n",
    "    data = []\n",
    "\n",
    "    # Find all tables that contain daily trading information\n",
    "    tables = soup.find_all(\"table\")\n",
    "\n",
    "    for table in tables:\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        for row in rows:\n",
    "            columns = row.find_all(\"td\")\n",
    "            if len(columns) == 12:  # Ensure the row has the expected number of columns\n",
    "                dia = columns[0].get_text(strip=True)\n",
    "                especif = columns[1].get_text(strip=True)\n",
    "\n",
    "                # Convert trades, quantity, and volume to int\n",
    "                trades = convert_to_int(columns[2].get_text(strip=True))\n",
    "                quantity = convert_to_int(columns[4].get_text(strip=True))\n",
    "                volume = convert_to_int(columns[5].get_text(strip=True))\n",
    "\n",
    "                # Convert open, low, high, average, close to float\n",
    "                open = convert_to_float(columns[7].get_text(strip=True))\n",
    "                low = convert_to_float(columns[8].get_text(strip=True))\n",
    "                high = convert_to_float(columns[9].get_text(strip=True))\n",
    "                average = convert_to_float(columns[10].get_text(strip=True))\n",
    "                close = convert_to_float(columns[11].get_text(strip=True))\n",
    "\n",
    "                # Filter out unwanted rows based on some patterns\n",
    "                if \"Negocia√ß√µes\" in dia or \"Dia\" in dia or \"Total\" in dia:\n",
    "                    continue  # Skip unwanted rows\n",
    "\n",
    "                # Combine 'dia' with 'mes' to create a full date\n",
    "                full_date = pd.to_datetime(f\"{dia}-{date}\", format=\"%d-%m-%Y\")\n",
    "\n",
    "                # Split 'Especif.' into stock code and listing\n",
    "                parts = especif.split()\n",
    "\n",
    "                # Map 'ticker_code' from the first part (stock_name) using stock_info\n",
    "                ticker_ = parts[0]\n",
    "                ticker = stock_info.get(\n",
    "                    ticker_, ticker_\n",
    "                )  # Map or fallback to stock_code\n",
    "                ticker_number = \"\".join([char for char in ticker if char.isdigit()])\n",
    "\n",
    "                # Extract 'listing' if available\n",
    "                listing = parts[1] if len(parts) > 1 else None\n",
    "\n",
    "                if listing == \"*\":\n",
    "                    trades = (\n",
    "                        trades * 10\n",
    "                    )  # (transformar lotes de mil em 10 lotes de 100)\n",
    "                    listing = \"\"\n",
    "\n",
    "                # Map 'listing' using the governance_levels dictionary\n",
    "                if listing in settings.governance_levels:\n",
    "                    listing = settings.governance_levels[listing]\n",
    "\n",
    "                # Append row data to the list\n",
    "                data.append(\n",
    "                    {\n",
    "                        \"company_name\": company_name,\n",
    "                        \"trading_name\": trading_name,\n",
    "                        \"ticker_code\": ticker_code + ticker_number,\n",
    "                        \"date\": full_date,\n",
    "                        \"listing\": listing,\n",
    "                        \"trades\": trades,\n",
    "                        \"quantity\": quantity,\n",
    "                        \"volume\": volume,\n",
    "                        \"open\": open,\n",
    "                        \"low\": low,\n",
    "                        \"high\": high,\n",
    "                        \"average\": average,\n",
    "                        \"close\": close,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    df = pd.DataFrame(data).drop_duplicates()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings.db_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_file = f\"..\\data\\{settings.db_filepath}\"\n",
    "\n",
    "conn = sqlite3.connect(db_file)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the date range from January 1996 to the current month\n",
    "start_date = \"1991-01\"\n",
    "# start_date = '1986-01'\n",
    "start_date = \"1997-01\"  # temp debug\n",
    "\n",
    "\n",
    "companies = get_companies_listing()\n",
    "pages = get_pages_to_download(companies, start_date)\n",
    "error_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tipo = \"RES_MERC_VISTA\"\n",
    "\n",
    "dfs = []\n",
    "\n",
    "start_time = time.time()\n",
    "for i, (ticker_code, company_name, trading_name, date) in enumerate(items):\n",
    "    if [ticker_code, date] not in error_url:\n",
    "\n",
    "        # debug skip\n",
    "        if \"BRSR\" not in ticker_code:\n",
    "            continue\n",
    "\n",
    "        # Fetch the webpage content\n",
    "        url = f\"https://bvmf.bmfbovespa.com.br/sig/FormConsultaMercVista.asp?strTipoResumo={tipo}&strSocEmissora={ticker_code}&strDtReferencia={date}&strIdioma=P&intCodNivel=2&intCodCtrl=160\"\n",
    "        try:\n",
    "            # get page response\n",
    "            response = requests.get(url, headers=header_random(), verify=False)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                # Parse the HTML using BeautifulSoup\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # Check if page has target content\n",
    "                stock_info = fetch_stock_info(soup)\n",
    "\n",
    "                # if page has target content, get data\n",
    "                if stock_info:\n",
    "                    df = fetch_trades(\n",
    "                        soup, stock_info, ticker_code, company_name, trading_name, date\n",
    "                    )\n",
    "                    dfs.append(df)\n",
    "\n",
    "                    # print status\n",
    "                    extra_info = [ticker_code, date, url]\n",
    "                    print_info(i, extra_info, start_time, len(items))\n",
    "                    if i == 129499:\n",
    "                        break\n",
    "\n",
    "                # if not target content, add to error list\n",
    "                else:\n",
    "                    error_url.append([ticker_code, date])\n",
    "\n",
    "                    # print status\n",
    "                    extra_info = [ticker_code, date]\n",
    "                    print_info(i, extra_info, start_time, len(items))\n",
    "\n",
    "            # if response error\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Failed to fetch the page. Status code: {response.status_code}, {i}\"\n",
    "                )\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error requesting {ticker_code}, {date}: {e}\")\n",
    "\n",
    "    # if already checked item for error\n",
    "    else:\n",
    "        print(ticker_code, date)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Example variables\n",
    "tipo = \"RES_MERC_VISTA\"\n",
    "ticker = \"BRSR\"  # Replace this with a value from companies['ticker']\n",
    "mes = \"04/2017\"\n",
    "\n",
    "# Build the base URL\n",
    "base_url = f\"https://bvmf.bmfbovespa.com.br/sig/FormConsultaMercVista.asp?strTipoResumo={tipo}&strSocEmissora={ticker}&strDtReferencia={mes}&strIdioma=P&intCodNivel=2&intCodCtrl=160\"\n",
    "\n",
    "# Fetch the webpage content\n",
    "try:\n",
    "    response = requests.get(base_url, headers=header_random(), verify=False)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        print(f\"URL: {base_url} - Status: {response.status_code} - Success\")\n",
    "\n",
    "        # Locate the tables by finding the table with id \"tblResDiario\"\n",
    "        tables = soup.find_all(\"table\", {\"id\": \"tblResDiario\"})\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error requesting {base_url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 == 1:\n",
    "    stock_info = {}\n",
    "\n",
    "    # Locate the rows with \"Nome da A√ß√£o\" to get stock name and ticker\n",
    "    stock_rows = soup.find_all(\"td\", class_=\"tittabela\", colspan=\"7\")\n",
    "\n",
    "    for row in stock_rows:\n",
    "        stock_text = row.get_text(strip=True)\n",
    "        if \"Nome da A√ß√£o\" in stock_text:\n",
    "            # Extract the stock name and ticker (in parentheses)\n",
    "            stock_name = stock_text.split(\"(\")[0].replace(\"Nome da A√ß√£o:\", \"\").strip()\n",
    "            stock_ticker = stock_text.split(\"(\")[1].replace(\")\", \"\").strip()\n",
    "\n",
    "            # Add stock_name and stock_ticker to the dictionary\n",
    "            stock_info[stock_name] = stock_ticker\n",
    "\n",
    "stock_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the table data\n",
    "if 1 == 1:\n",
    "\n",
    "# Create a DataFrame and remove duplicates\n",
    "df = pd.DataFrame(data).drop_duplicates()\n",
    "\n",
    "# Display the DataFrame\n",
    "df.iloc[:60]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
